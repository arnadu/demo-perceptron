{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "324e609d-b763-4aba-8ad7-0d1b9e1745d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://colab.research.google.com/drive/1ExE__T9e2dMDKbxrJfgp8jP0So8umC-A#sandboxMode=true&scrollTo=OvvBAoQVJsuU\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992547e2-5991-4461-b221-b4de140fe079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nenv = gym.make('CartPole-v1')\\nnum_features = env.observation_space.shape[0]\\nnum_actions = env.action_space.n\\nprint('Number of state features: {}'.format(num_features))\\nprint('Number of possible actions: {}'.format(num_actions))\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "env = gym.make('CartPole-v1')\n",
    "num_features = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "print('Number of state features: {}'.format(num_features))\n",
    "print('Number of possible actions: {}'.format(num_actions))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e180df02-6656-433c-ae39-a81cb8a901e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of state features: 8\n",
      "Number of possible actions: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remyh\\AppData\\Roaming\\Python\\Python38\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "import gym_examples\n",
    "env = gym.make('gym_examples/PuckWorld-v0', reward1=True, reward2=True, render_mode=None, fps=60)  #'human'\n",
    "env._max_episode_steps = 300\n",
    "#num_actions = 5 #PuckWorld: 0,1,2,3,4 represent left, right, up, down, -, five moves.\n",
    "num_features = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "print('Number of state features: {}'.format(num_features))\n",
    "print('Number of possible actions: {}'.format(num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ca2c4f-ecc2-47e2-baa0-a9eb441b237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(tf.keras.Model):\n",
    "    \"\"\"Dense neural network class.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.dense2 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.dense3 = tf.keras.layers.Dense(num_actions, dtype=tf.float32) # No activation\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "main_nn = DQN()\n",
    "target_nn = DQN()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "mse = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d53e91-b1b0-4778-b422-ec491bf35e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"Experience replay buffer that samples uniformly.\"\"\"\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        idx = np.random.choice(len(self.buffer), num_samples)\n",
    "        for i in idx:\n",
    "            elem = self.buffer[i]\n",
    "            state, action, reward, next_state, done = elem\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones, dtype=np.float32)\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67fd4c2a-0642-4531-8c4d-2a3c632f2324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epsilon_greedy_action(state, epsilon):\n",
    "    \"\"\"Take random action with probability epsilon, else take best action.\"\"\"\n",
    "    result = tf.random.uniform((1,))\n",
    "    if result < epsilon:\n",
    "        return env.action_space.sample() # Random action (left or right).\n",
    "    else:\n",
    "        return tf.argmax(main_nn(state)[0]).numpy() # Greedy action for state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc56e0b4-03bb-484b-ae5e-ef60b96501b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(states, actions, rewards, next_states, dones):\n",
    "    \"\"\"Perform a training iteration on a batch of data sampled from the experience\n",
    "    replay buffer.\"\"\"\n",
    "    # Calculate targets.\n",
    "    next_qs = target_nn(next_states)\n",
    "    max_next_qs = tf.reduce_max(next_qs, axis=-1)\n",
    "    target = rewards + (1. - dones) * discount * max_next_qs\n",
    "    with tf.GradientTape() as tape:\n",
    "        qs = main_nn(states)\n",
    "        action_masks = tf.one_hot(actions, num_actions)\n",
    "        masked_qs = tf.reduce_sum(action_masks * qs, axis=-1)\n",
    "        loss = mse(target, masked_qs)\n",
    "    grads = tape.gradient(loss, main_nn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, main_nn.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b58f681-3a24-4a92-a03a-fe6f161eb49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters.\n",
    "num_episodes = 1000\n",
    "epsilon = 1.0\n",
    "batch_size = 32\n",
    "discount = 0.99\n",
    "buffer = ReplayBuffer(100000)\n",
    "cur_frame = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30936c57-d052-4ad1-9d0a-1028ab6387e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remyh\\AppData\\Roaming\\Python\\Python38\\site-packages\\gym\\utils\\passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\remyh\\AppData\\Roaming\\Python\\Python38\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "C:\\Users\\remyh\\AppData\\Roaming\\Python\\Python38\\site-packages\\gym\\utils\\passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\remyh\\AppData\\Roaming\\Python\\Python38\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/1000. Epsilon: 0.999. Reward in last 100 episodes: -228.019\n",
      "Episode 50/1000. Epsilon: 0.949. Reward in last 100 episodes: -206.034\n",
      "Episode 100/1000. Epsilon: 0.899. Reward in last 100 episodes: -200.094\n",
      "Episode 150/1000. Epsilon: 0.849. Reward in last 100 episodes: -189.215\n",
      "Episode 200/1000. Epsilon: 0.799. Reward in last 100 episodes: -185.613\n",
      "Episode 250/1000. Epsilon: 0.749. Reward in last 100 episodes: -181.898\n",
      "Episode 300/1000. Epsilon: 0.699. Reward in last 100 episodes: -176.589\n",
      "Episode 350/1000. Epsilon: 0.649. Reward in last 100 episodes: -169.243\n",
      "Episode 400/1000. Epsilon: 0.599. Reward in last 100 episodes: -161.341\n",
      "Episode 450/1000. Epsilon: 0.549. Reward in last 100 episodes: -151.352\n",
      "Episode 500/1000. Epsilon: 0.499. Reward in last 100 episodes: -134.186\n",
      "Episode 550/1000. Epsilon: 0.449. Reward in last 100 episodes: -121.433\n",
      "Episode 600/1000. Epsilon: 0.399. Reward in last 100 episodes: -114.275\n",
      "Episode 650/1000. Epsilon: 0.349. Reward in last 100 episodes: -106.030\n",
      "Episode 700/1000. Epsilon: 0.299. Reward in last 100 episodes: -95.666\n",
      "Episode 750/1000. Epsilon: 0.249. Reward in last 100 episodes: -94.826\n",
      "Episode 800/1000. Epsilon: 0.199. Reward in last 100 episodes: -91.967\n",
      "Episode 850/1000. Epsilon: 0.149. Reward in last 100 episodes: -91.208\n",
      "Episode 900/1000. Epsilon: 0.099. Reward in last 100 episodes: -96.654\n",
      "Episode 950/1000. Epsilon: 0.050. Reward in last 100 episodes: -95.052\n",
      "Episode 1000/1000. Epsilon: 0.050. Reward in last 100 episodes: -100.297\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Start training. Play game once and then train with a batch.\n",
    "last_100_ep_rewards = []\n",
    "for episode in range(num_episodes+1):\n",
    "    state, info = env.reset()  \n",
    "    ep_reward, done, truncated = 0, False, False\n",
    "    while not (done or truncated):\n",
    "        state_in = tf.expand_dims(state, axis=0)\n",
    "        action = select_epsilon_greedy_action(state_in, epsilon)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        ep_reward += reward\n",
    "        # Save to experience replay.\n",
    "        buffer.add(state, action, reward, next_state, done or truncated)\n",
    "        state = next_state\n",
    "        cur_frame += 1\n",
    "        # Copy main_nn weights to target_nn.\n",
    "        if cur_frame % 2000 == 0:\n",
    "            target_nn.set_weights(main_nn.get_weights())\n",
    "\n",
    "        # Train neural network.\n",
    "        if len(buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "            loss = train_step(states, actions, rewards, next_states, dones)\n",
    "  \n",
    "    if episode < 950:\n",
    "        epsilon -= 0.001\n",
    "\n",
    "    if len(last_100_ep_rewards) == 100:\n",
    "        last_100_ep_rewards = last_100_ep_rewards[1:]\n",
    "    last_100_ep_rewards.append(ep_reward)\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        print(f'Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}. '\n",
    "            f'Reward in last 100 episodes: {np.mean(last_100_ep_rewards):.3f}')\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b638c-cf30-4a36-9eea-693818835431",
   "metadata": {},
   "outputs": [],
   "source": [
    "envh = gym.make('gym_examples/PuckWorld-v0', reward1=True, reward2=True, render_mode='human', fps=60)  #'human'\n",
    "#envh = env = gym.make('CartPole-v1', render_mode='human')\n",
    "state, info = envh.reset()\n",
    "done, truncated = False, False\n",
    "ep_rew = 0\n",
    "while not done or truncated:\n",
    "    envh.render()\n",
    "    state = tf.expand_dims(state, axis=0)\n",
    "    action = select_epsilon_greedy_action(state, epsilon=0.01)\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    ep_rew += reward\n",
    "print('Episode reward was {}'.format(ep_rew))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1a158-0b8a-432f-a084-3381b8f1f7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
