{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbb60d3",
   "metadata": {},
   "source": [
    "<h1>A toy implementation of a one-hidden layer classifier</h1>\n",
    "\n",
    "loss function = cross-entropy<br>\n",
    "output layer = softmax<br>\n",
    "hidden layer = relu<br>\n",
    "\n",
    "![Simple Classifier](img/one_layer_classifier.drawio.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfac2d2-fcc1-4069-a8b5-596946c418d1",
   "metadata": {},
   "source": [
    "<h1>Cross entropy loss function for the classification of a batch of several samples </h1>\n",
    "\n",
    "$ Loss = - \\sum\\limits _{i=0}^{Samples-1} \\sum\\limits_{l=0}^{Categories-1} y^{train}_{il} . log(logits_{il})  $\n",
    "\n",
    "$Samples$: the number of samples in the batch <br>\n",
    "$Categories$ : the number of categories <br>\n",
    "$y_{il}$ the truth value for the i-th sample and the l-th category, as a one-hot encoded vector over the possible categories<br>\n",
    "$logits_{il} = h^2_{il}$ the predicted probability for l-th category of the i-th sample (a vector of probabilities ]0,1] for the i-th sample). The logits are the output of the network <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c8695db-a7c6-4366-9983-c7f65ebe3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c14ba29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.513306124309698, array([0.10536052, 2.30258509, 0.10536052]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def cross_entropy_loss(y, logits):\n",
    "    \n",
    "    \"\"\"the cross entropy loss for a batch of samples\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    y: np.array[num_samples, num_categories] \n",
    "        for each sample, a one-hot encoded vector with 1 for the true category and zero everywhere else\n",
    "    \n",
    "    logits: np.array[num_samples, num_categories]\n",
    "        for each sample, the probabily vector for the corresponding categories\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    batch_loss: scalar\n",
    "        the total cross entropy loss for the entire batch of samples\n",
    "    \n",
    "    individual_losses: np.array[num_samples]\n",
    "        the cross entropy loss for each sample\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    \n",
    "    individual_losses[i] = - Sum_l { Y[i,l] * ln(logits[i,l]) }\n",
    "    \n",
    "    batch_loss = Sum_i { individual_losses[i] }\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = -np.log(logits)*y  #calculate y*log(p) for each category of each sample of the batch loss.shape is (i,l)\n",
    "    individual_losses = np.sum(loss, -1)  #sum over categories\n",
    "    batch_loss = np.sum(individual_losses) #sum over samples\n",
    "    return batch_loss, individual_losses\n",
    "\n",
    "#test\n",
    "#------\n",
    "#prepare a batch of 3 samples with 2 categories\n",
    "y = np.array([[0,1], [1,0], [1,0]])   #one-hot encoded category of each sample\n",
    "logits = np.array([[0.1,0.9], [0.1,0.9], [0.9,0.1]])  #predicted proba of each category for each sample\n",
    "\n",
    "cross_entropy_loss(y, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310f87ad",
   "metadata": {},
   "source": [
    "<h1>SOFTMAX function as output layer</h1>\n",
    "\n",
    "The SOFTMAX functions ensures that the output values look like a vector of probabilities:<br>\n",
    "<li>Each value is within ]0,1[</li>\n",
    "<li>They sum exactly to 1</li>\n",
    "<br>\n",
    "\n",
    "The function SOFTMAX_l returns the value of the l-th logit:\n",
    "\n",
    "$ SOFTMAX_l(z_1, ... z_l, ..., z_{num_categories}) =  e^{z_i} / \\sum\\limits_{l'=0}^{Categories-1}e^{z_{l'}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f1f4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.53978687e-05, 9.99954602e-01],\n",
       "       [5.00000000e-01, 5.00000000e-01],\n",
       "       [9.93307149e-01, 6.69285092e-03]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"SOFTMAX function\n",
    "        calculates the logits for a batch of samples\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z: np.array[num_samples, num_categories]\n",
    "        for each sample, the vector of values that need to be normalized into a vector of probabilities\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    softmax: np.array[num_samples, num_categories]\n",
    "        for each sample, a normalized vector of probabilities\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    The input values are normalized to avoid floating point exceptions when calling the exponentional function\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = np.max(z)   #normalize to avoid large numbers \n",
    "    e = np.exp(z - m)\n",
    "    return e / e.sum(axis=1, keepdims=True) #[:,None]\n",
    "\n",
    "#test\n",
    "#----\n",
    "#call on a batch of 3 samples with 2 categories\n",
    "z = np.array([[0, 10.], [0.,0.], [0, -5]])  #predicted proba of each category for each sample\n",
    "softmax(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0226e52-6868-4082-a8ad-e3464a99ae63",
   "metadata": {},
   "source": [
    "<h1>The Rectified Linear Unit (ReLU)</h1>\n",
    "\n",
    "The activition function for the neurons in the hidden layer\n",
    "\n",
    "relu(z) = MAX(0,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1207af67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [2., 0., 3.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def relu(v):\n",
    "    \n",
    "    \"\"\"Rectified Linear Unit (ReLU) function for a batch of samples\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        z: np.array[num_samples, num_neurons]\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "        h: np.array[num_samples, num_neurons]\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.where(v>0., v, 0.)\n",
    "\n",
    "#test\n",
    "z = np.array([[-1.,0.,1.,],[2.,-10.,3.,]])\n",
    "relu(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3836ea59-1332-40e3-977a-2caa0f5c4902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier():\n",
    "    \"\"\"A naive implementation of a 1-layer perceptron classifier\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer1_dim, seed=1, num_periods=100, alpha=0.1):\n",
    "        \"\"\"set the hyper parameters\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        layer1_dim: int\n",
    "            the number of neurons in the hidden-layer\n",
    "\n",
    "        num_periods: int\n",
    "            the number of timesteps used for training the network\n",
    "\n",
    "        alpha: float\n",
    "            the learning rate\n",
    "        \"\"\"\n",
    "        \n",
    "        self.layer1_dim = layer1_dim\n",
    "        self.seed = seed\n",
    "        self.num_periods = num_periods\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"forward pass\n",
    "            call fit(X_train, Y_train) before using this method\n",
    "        \n",
    "        Paramaters\n",
    "        ----------\n",
    "        X: np.array[num_samples, num_features]\n",
    "            a batch of samples that need to be categorized\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        logits: np.array[num_sampples, num_categories]\n",
    "            the probability vector corresponding to the possible categories, for each sample of the batch\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "\n",
    "        This function calculates the internal state of the neural network and saves it in the following variables\n",
    "        \n",
    "        z1_: np.array[num_samples, layer1_dim]\n",
    "            W1.X+B1\n",
    "            \n",
    "        h1_: np.array[num_samples, layer1_dim]\n",
    "            Relu(z1)\n",
    "\n",
    "        z2_: np.array[num_samples, layer2_dim]\n",
    "            W2.H1+B2\n",
    "            \n",
    "        h2_: np.array[num_samples, layer2_dim]\n",
    "            the output layer, the estimated logits calculated as Softmax(z2)\n",
    "        \"\"\"\n",
    "        \n",
    "        #matmul will return a matrix of size SxL1 where L1 is size of layer1\n",
    "        self.z1_ = np.matmul(X,self.w1_) + self.b1_\n",
    "        self.h1_ = relu(self.z1_)  #shape=(S,L1)\n",
    "\n",
    "        self.z2_ = np.matmul(self.h1_, self.w2_) + self.b2_  #shape=(S,C) where C is size of output=number of categories\n",
    "        self.h2_ = softmax(self.z2_)  #shape=(S,C)\n",
    "\n",
    "        return self.h2_\n",
    "        \n",
    "    def score(self, X, Y):\n",
    "        \"\"\"compute the loss for a batch of samples\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        total_loss: float\n",
    "            the total loss over the entire batch\n",
    "        samples_losses: np.array[num_samples]\n",
    "            the cross-entropy-loss for each sample in the batch\n",
    "        \"\"\"\n",
    "        \n",
    "        pred = self.predict(X)\n",
    "        self.total_loss_, self.samples_losses_ = cross_entropy_loss(Y, pred)  #loss scalar, _ is vector of size S, total loss for entire batch, loss for each sample of batch\n",
    "        return self.total_loss_\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"train the network on a training set\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: np.array[num_samples, num_features]\n",
    "            a batch of samples that need to be categorized\n",
    "\n",
    "        Y: np.array[num_samples, num_categories]\n",
    "            the corresponding truth values, encoded as one-hot vector over the possible categories\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        self\n",
    "        \n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        \n",
    "        X_train = np.array([[0,0,1], \n",
    "                            [0,1,0],\n",
    "                            [0,1,1],\n",
    "                            [1,0,0],\n",
    "                            [1,0,1]])\n",
    "\n",
    "        Y_train = np.array([[0,1],\n",
    "                            [1,0],\n",
    "                            [0,1],\n",
    "                            [1,0],\n",
    "                            [0,1]])\n",
    "\n",
    "        m = Classifier(4)\n",
    "        \n",
    "        m.fit(X_train, Y_train)\n",
    "        \n",
    "        logits = m.predict(X_train)\n",
    "        print(logits)\n",
    "        \n",
    "        m.score(X_train, Y_train)\n",
    "        print(m.total_loss_, m.samples_losses_)\n",
    "\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "\n",
    "        This function calculates the weights and biases of the neural network and saves it in the following variables\n",
    "        \n",
    "        b1_: np.array[layer1_dim]  \n",
    "            biais of the hidden layer\n",
    "            \n",
    "        w1_: np.array[input_dim, layer1_dim]\n",
    "            weights of the hiddent layer\n",
    "            \n",
    "        b2_: np.array[output_dim]\n",
    "            biais of the output layer\n",
    "            \n",
    "        w2_: np.array[layer1_dim, output_dim]\n",
    "            weigths of the output layer\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #automatically figure out the size of the input vectors as well as the number of possible categories\n",
    "        self.input_dim = X.shape[1] #number of input features\n",
    "        self.output_dim = Y.shape[1] #number of classes\n",
    "        \n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        #intializae weights and biases with random value\n",
    "        self.w1_ = 2*np.random.random((self.input_dim , self.layer1_dim)) - 1\n",
    "        self.b1_ = 2*np.random.random(self.layer1_dim) - 1\n",
    "        self.w2_ = 2*np.random.random((self.layer1_dim, self.output_dim)) - 1\n",
    "        self.b2_ = 2*np.random.random(self.output_dim) - 1\n",
    "        \n",
    "        for p in range(self.num_periods):\n",
    "\n",
    "            batch_X = X #[0:1,:]\n",
    "            batch_Y = Y #[0:1,:]\n",
    "\n",
    "            #forward pass to get the current h's and z's\n",
    "            pred = self.predict(batch_X)\n",
    "\n",
    "            #compute gradients\n",
    "            self.g_b2_ = np.zeros_like(self.b2_)\n",
    "            self.g_w2_ = np.zeros_like(self.w2_)\n",
    "            self.g_b1_ = np.zeros_like(self.b1_)\n",
    "            self.g_w1_ = np.zeros_like(self.w1_)\n",
    "\n",
    "            g_loss_softmax = -(batch_Y - self.h2_)  #shape=S,C\n",
    "\n",
    "            self.g_b2_ = np.sum(g_loss_softmax, axis=0) #shape C; sum(axis=0) is sum over samples\n",
    "\n",
    "            self.g_w2_ = np.einsum(\"ij,ik\", self.h1_, g_loss_softmax)\n",
    "\n",
    "            dRelu = np.where(self.z1_>0., 1., 0.)  #shape=S,L1\n",
    "\n",
    "            self.g_b1_ = np.einsum(\"il,kl,ik->k\", g_loss_softmax, self.w2_, dRelu)\n",
    "\n",
    "            self.g_w1_ += np.einsum(\"il,kl,ik,ij->jk\", g_loss_softmax, self.w2_, dRelu, batch_X)\n",
    "\n",
    "            #adjust weights and biases in the direction to reduce the loss\n",
    "            batch_size = batch_X.shape[0]\n",
    "            self.b2_ -= self.alpha * self.g_b2_ / batch_size #average over number of samples\n",
    "            self.w2_ -= self.alpha * self.g_w2_ / batch_size\n",
    "            self.b1_ -= self.alpha * self.g_b1_ / batch_size\n",
    "            self.w1_ -= self.alpha * self.g_w1_ / batch_size            \n",
    "\n",
    "        return self\n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a80c1d7-f293-4b97-bec0-2b4e460cb0ad",
   "metadata": {},
   "source": [
    "<h1>A simple test case</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fdd83c0-a818-4e79-83d9-c5ac5610f675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.585585397760034 [0.018467   0.26716293 0.04445894 0.19173101 0.06376552]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[0,0,1], \n",
    "                    [0,1,0],\n",
    "                    [0,1,1],\n",
    "                    [1,0,0],\n",
    "                    [1,0,1]])\n",
    "\n",
    "Y_train = np.array([[0,1],\n",
    "                    [1,0],\n",
    "                    [0,1],\n",
    "                    [1,0],\n",
    "                    [0,1]])\n",
    "\n",
    "m = Classifier(4)\n",
    "m.fit(X_train, Y_train)\n",
    "m.score(X_train, Y_train)\n",
    "print(m.total_loss_, m.samples_losses_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a647a34-bf88-496b-9828-af2b04562483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.0360167415092\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "S=100 #number of samples\n",
    "\n",
    "X1 = 2*np.random.rand(S,3) - 1\n",
    "\n",
    "Y1 = np.zeros((S,2))\n",
    "Y1[:,0] = np.where(X1[:,0]+X1[:,1]+X1[:,2]>0,1,0)\n",
    "Y1[:,1] = np.where(X1[:,0]+X1[:,1]+X1[:,2]>0,0,1)\n",
    "\n",
    "\n",
    "m = Classifier(layer1_dim=3, seed=1, num_periods=50, alpha=0.1)\n",
    "\n",
    "m.fit(X1, Y1)\n",
    "\n",
    "m.score(X1, Y1)\n",
    "\n",
    "print(m.total_loss_)\n",
    "\n",
    "pred = m.h2_\n",
    "\n",
    "marker = np.where(pred[:,0]>=0.5,\"o\",\"s\")\n",
    "\n",
    "color = np.where(Y1[:,0]>0,\"green\",\"blue\")\n",
    "\n",
    "color = np.where((Y1[:,0]>0) & (pred[:,0]<0.5) | (Y1[:,1]>0) & (pred[:,1]<0.5),\"red\",color)\n",
    "\n",
    "#plot the results\n",
    "#marker are predicted values; colors are true values with red showing wrong classification\n",
    "fig = plt.figure()\n",
    "#ax = fig.add_subplot(projection='3d')\n",
    "ax = fig.add_subplot()\n",
    "for i in range(X1.shape[0]):\n",
    "    ax.scatter(X1[i,0],X1[i,1],c=color[i],marker=marker[i],alpha=0.5)\n",
    "    ax.scatter(X1[i,0],X1[i,1],c=color[i],marker=marker[i],alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9510dbf-067b-4be7-bce9-4bcb9b822850",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "\n",
    "The objective of training is to optimize the weights and biais (given hyperparameters such as the number of input features, the number of neurons in the hidden layer and the number of categories) in order so as to minimize the loss function on as broad a range of inputs as possible.\n",
    "\n",
    "Given a training set X, Y we are looking for\n",
    "$\\hat{w^1},\\hat{b^1},\\hat{w^2},\\hat{b^2} = \\underset{w^1,b^1,w^2,b^2}{argmin}Loss(X,Y)$\n",
    "\n",
    "\n",
    "The procedure implemented here is very simplistic:\n",
    "- initiate the weights with random values\n",
    "- iterate for a given numer of times:\n",
    "    - calculate the current loss on the training set, and its gradient to all the parameters\n",
    "    - adjust the weights and biais in the direction that diminishes the loss according to the gradient\n",
    "\n",
    "\n",
    "The main difficulty with this procedure is to compute the gradient of the loss function with respect to the weights, in order to find the direction in which to adjust the parameters<br>\n",
    "\n",
    "A 'forward' differentiation by perturbing the values and recomputing the loss is possible but would be pretty slow.\n",
    "\n",
    "The 'backward propagation' method is more efficient. It is based on the mathematical rule of differentiation, applied to the forward calculation of the neural network seen as a mathematical function:\n",
    "$$\n",
    "Loss = \\frac{1}{num samples}\\sum_{i=0}^{num samples-1}CrossEntropyLoss(X_i, Y_i, W^1, B^1, W^2, B^2)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afadbe3c-b45a-43d6-8097-bf1d64c9a66d",
   "metadata": {},
   "source": [
    "## pre-requisite: the Softmax derivative\n",
    "\n",
    "It helps to calculate the derivative of the log of the softmax and to use $\\frac{\\partial log(softmax)}{\\partial z}=\\frac{1}{softmax}\\frac{\\partial softmax}{\\partial z}$ to obtain $\\frac{\\partial softmax}{\\partial z} = softmax . \\frac{\\partial log(softmax)}{\\partial z}$\n",
    "\n",
    "For a given logit $h^2_l=S_l$ its derivative with respect to one of C possible inputs $z_l'$ is obtained as:\n",
    "\n",
    "$$S_l(z_0, ... z_{C-1}) = \\frac{e^{z_l}}{\\sum_{l''=0}^{C-1} e^{z_{l''}}}$$\n",
    "\n",
    "$$log [S_l(z_0, ... z_{C-1})] = z_l - log(\\sum_{l''=0}^{C-1} e^{z_{l''}})$$\n",
    "\n",
    "therefore\n",
    "\n",
    "$$\\frac{\\partial log [S_l(z_0, ... z_C)]}{\\partial z_l'} = \\delta_{l=l'} - \\frac{1}{\\sum_{l''=0}^{C-1} e^{z_{l''}}} . e^{z_{l'}} = \\delta_{l=l'} - S_{l'}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{\\partial S_l}{\\partial z_{l'}} = S_l . ( \\delta_{l=l'} - S_{l'})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b37d54-e423-4060-a3e0-e93ba28bb906",
   "metadata": {},
   "source": [
    "## pre-requisite: the Cross Entropy Loss derivative\n",
    "\n",
    "We will also be interested in getting $\\frac{\\partial CEL}{\\partial z_{l'}}$ with $$CEL=-\\sum_{l=0}^{C-1}y_l . log[S_{l}]$$\n",
    "\n",
    "One obtains first:\n",
    "\n",
    "$$\\frac{\\partial CEL}{\\partial z_{l'}} = - \\sum_{l=0}^{C-1} \\frac{y_l}{S_l} \\frac{\\partial S_l}{\\partial z_{l'}}$$\n",
    "\n",
    "and by using the result for the derivative of the Softmax, the result:\n",
    "\n",
    "$$\\frac{\\partial CEL}{\\partial z_{l'}} = - \\sum_{l=0}^{C-1} y_l . ( \\delta_{l=l'} - S_{l'} )$$\n",
    "\n",
    "And using the property of the softmax that Y is a one-hot encoded vector (or more precisely that its components sum to 1)\n",
    "\n",
    "$$ \\frac{\\partial CEL}{\\partial z_{l'}} = S_{l'} - y_{l'} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d407315-acd0-45b3-a813-48c65f8509e8",
   "metadata": {},
   "source": [
    "## Gradient with respect to the $b^2_l$:\n",
    "$$ \n",
    "\\frac{\\partial L}{\\partial b^2_{l'}} = \\sum_{i=0}^{num samples-1} \\frac{\\partial CEL_i}{\\partial b^2_{l'}}\n",
    "$$\n",
    "where $CEL_i$ is the cross-entropy-loss corresponding to sample i\n",
    "<br>\n",
    "\n",
    "$$\\frac{\\partial CEL_i}{\\partial b^2_{l'}} = \\sum_{l''=0}^{num categories-1} \\frac{\\partial CEL_i}{\\partial z^2_{il''}} . \\frac{\\partial z^2_{il''}}{\\partial b^2_{l'}}  = \\sum_{l''=0}^{num categories-1} (S_{il''} - y_{il''}) . \\frac{\\partial z^2_{il''}}{\\partial b^2_{l'}} $$\n",
    "\n",
    "which simplifies to (substituting $h^2_{l''}$ for the softmax $S_il$, and considering that the derivative of $z^2_{l''}$ to $b^2_{l'}$ is trivially 1 when $l'=l''$ and 0 otherwise\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b^2_{l'}} = \\sum_{i=0}^{num samples-1} (h^2_{il'} - y^{train}_{il'})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64079f17-5c6f-4532-a116-90e58c5cef22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Gradient with respect to the $w^2_l$:\n",
    "\n",
    "Using the results above:\n",
    "\n",
    "$$\\frac{\\partial CEL_i}{\\partial w^2_{kl}} = \\sum_{l'=0}^{num categories-1} (h^2_{il'} - y_{il'}) . \\frac{\\partial z^2_{il'}}{\\partial w^2_{kl}} $$\n",
    "\n",
    "with this time $\\frac{\\partial z^2_{il'}}{\\partial w^2_{kl}} = h^1_{kl} $ if $l=l'$ and 0 otherwise\n",
    "\n",
    "Therefore the gradient (matrix) is \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w^2_{kl}} = \\sum_{i=0}^{num samples-1} (h^2_{il} - y^{train}_{il}) . h^1_{ik}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed6aac2-c80f-4189-baf6-44378f1aa90a",
   "metadata": {},
   "source": [
    "## Gradient with respect to the $b^1_k$:\n",
    "\n",
    "$$\\frac{\\partial CEL_i}{\\partial b^1_k} = \\sum_{l'=0}^{num categories-1} (h^2_{il'} - y_{il'}) . \\frac{\\partial z^2_{il'}}{\\partial b^1_k} $$\n",
    "\n",
    "Remembering that \n",
    "\n",
    "$$ z^2_{il'} = \\sum_{k'=0}^{num neurons-1} w^2_{k'l'}.h^1_{ik'} + b^2_{l'}$$\n",
    "\n",
    "and that $$h^1_{ik'} = RELU(z^1_{ik'})$$\n",
    "\n",
    "One obtains\n",
    "\n",
    "$$\\frac{\\partial z^2_{il'}}{\\partial b^1_k} = \\sum_{k'=0}^{num neurons-1} w^2_{k'l'}  \\frac{\\partial h^1_{ik'}}{\\partial b^1_k} = \\sum_{k'=0}^{num neurons-1} w^2_{k'l'}  \\frac{\\partial RELU^1_{ik'}}{\\partial z^1_{ik'}} \\frac{\\partial z^1_{ik'}}{\\partial b^1_k}  $$\n",
    " \n",
    "Which simplifies to (because $dz_k'/db_k=0$ if $l \\ne l'$ and 1 else\n",
    "\n",
    "$$\\frac{\\partial z^2_{il'}}{\\partial b^1_k} =  w^2_{kl'}  \\frac{\\partial RELU^1_{ik}}{\\partial z^1_{ik}}  = w^2_{kl'} . \\delta_{z^1_{ik}>0}   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e67bb7d-9df4-4765-918d-846e831f52f6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b^1_{k}} = \\sum_{i=0}^{num samples-1} \\sum_{l'=0}^{num categories-1} \\sum_{k'=0}^{num neurons-1} (h^2_{il'} - y^{train}_{il'}) . h^1_{ik'} . w^2_{k'l'}  . \\delta_{z^1_{ik'}>0}  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf32e3-169a-4a8f-876c-a08d9f2ee1ab",
   "metadata": {},
   "source": [
    "## Gradient with respect to the $w^1_{kj}$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w^1_{kj}} = \\sum_{i=0}^{num samples-1} \\frac{\\partial CEL_i}{\\partial w^1_{kj}}$$\n",
    "\n",
    "$$\\frac{\\partial CEL_i}{\\partial w^1_{kj}} = \\sum_{l'=0}^{num categories-1} \\sum_{l'=0}^{num neurons-1} (h^2_{il'} - y_{il'}) . w^2_{k'l'} . \\frac{\\partial RELU^1_{ik'}}{\\partial z^1_{ik'}} \\frac{\\partial z^1_{ik'}}{\\partial w^1_{kj}}  $$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w^1_{kj}} = \\sum_{i=0}^{num samples-1} \\sum_{l'=0}^{num categories-1} \\sum_{k'=0}^{num neurons-1} (h^2_{il'} - y^{train}_{il'}) . h^1_{ik'} . w^2_{k'l'} . \\delta_{z^1_{ik'}>0} . x^{train}_{ij'}  $$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
