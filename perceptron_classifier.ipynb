{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbb60d3",
   "metadata": {},
   "source": [
    "<h1>A toy implementation of a one-hidden layer classifier</h1>\n",
    "\n",
    "loss function = cross-entropy<br>\n",
    "output layer = softmax<br>\n",
    "hidden layer = relu<br>\n",
    "\n",
    "![Simple Classifier](img/one_layer_classifier.drawio.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfac2d2-fcc1-4069-a8b5-596946c418d1",
   "metadata": {},
   "source": [
    "<h1>Cross entropy loss function for the classification of a batch of several samples </h1>\n",
    "\n",
    "$ Loss = - \\sum\\limits _{i=0}^{Samples-1} \\sum\\limits_{l=0}^{Categories-1} y^{train}_{il} . log(logits_{il})  $\n",
    "\n",
    "$Samples$: the number of samples in the batch <br>\n",
    "$Categories$ : the number of categories <br>\n",
    "$y_{il}$ the truth value for the i-th sample and the l-th category, as a one-hot encoded vector over the possible categories<br>\n",
    "$logits_{il} = h^2_{il}$ the predicted probability for l-th category of the i-th sample (a vector of probabilities ]0,1] for the i-th sample). The logits are the output of the network <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c8695db-a7c6-4366-9983-c7f65ebe3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c14ba29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.513306124309698, array([0.10536052, 2.30258509, 0.10536052]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def cross_entropy_loss(y, logits):\n",
    "    \n",
    "    \"\"\"the cross entropy loss for a batch of samples\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    y: np.array[num_samples, num_categories] \n",
    "        for each sample, a one-hot encoded vector with 1 for the true category and zero everywhere else\n",
    "    \n",
    "    logits: np.array[num_samples, num_categories]\n",
    "        for each sample, the probabily vector for the corresponding categories\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    batch_loss: scalar\n",
    "        the total cross entropy loss for the entire batch of samples\n",
    "    \n",
    "    individual_losses: np.array[num_samples]\n",
    "        the cross entropy loss for each sample\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    \n",
    "    individual_losses[i] = - Sum_l { Y[i,l] * ln(logits[i,l]) }\n",
    "    \n",
    "    batch_loss = Sum_i { individual_losses[i] }\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = -np.log(logits)*y  #calculate y*log(p) for each category of each sample of the batch loss.shape is (i,l)\n",
    "    individual_losses = np.sum(loss, -1)  #sum over categories\n",
    "    batch_loss = np.sum(individual_losses) #sum over samples\n",
    "    return batch_loss, individual_losses\n",
    "\n",
    "#test\n",
    "#------\n",
    "#prepare a batch of 3 samples with 2 categories\n",
    "y = np.array([[0,1], [1,0], [1,0]])   #one-hot encoded category of each sample\n",
    "logits = np.array([[0.1,0.9], [0.1,0.9], [0.9,0.1]])  #predicted proba of each category for each sample\n",
    "\n",
    "cross_entropy_loss(y, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310f87ad",
   "metadata": {},
   "source": [
    "<h1>SOFTMAX function as output layer</h1>\n",
    "\n",
    "The SOFTMAX functions ensures that the output values look like a vector of probabilities:<br>\n",
    "<li>Each value is within ]0,1[</li>\n",
    "<li>They sum exactly to 1</li>\n",
    "<br>\n",
    "\n",
    "The function SOFTMAX_l returns the value of the l-th logit:\n",
    "\n",
    "$ SOFTMAX_l(z_1, ... z_l, ..., z_{num_categories}) =  e^{z_i} / \\sum\\limits_{l'=0}^{Categories-1}e^{z_{l'}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02f1f4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.53978687e-05, 9.99954602e-01],\n",
       "       [5.00000000e-01, 5.00000000e-01],\n",
       "       [9.93307149e-01, 6.69285092e-03]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"SOFTMAX function\n",
    "        calculates the logits for a batch of samples\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z: np.array[num_samples, num_categories]\n",
    "        for each sample, the vector of values that need to be normalized into a vector of probabilities\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    softmax: np.array[num_samples, num_categories]\n",
    "        for each sample, a normalized vector of probabilities\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    The input values are normalized to avoid floating point exceptions when calling the exponentional function\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = np.max(z)   #normalize to avoid large numbers \n",
    "    e = np.exp(z - m)\n",
    "    return e / e.sum(axis=1, keepdims=True) #[:,None]\n",
    "\n",
    "#test\n",
    "#----\n",
    "#call on a batch of 3 samples with 2 categories\n",
    "z = np.array([[0, 10.], [0.,0.], [0, -5]])  #predicted proba of each category for each sample\n",
    "softmax(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0226e52-6868-4082-a8ad-e3464a99ae63",
   "metadata": {},
   "source": [
    "<h1>The Rectified Linear Unit (ReLU)</h1>\n",
    "\n",
    "The activition function for the neurons in the hidden layer\n",
    "\n",
    "relu(z) = MAX(0,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1207af67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [2., 0., 3.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def relu(v):\n",
    "    \n",
    "    \"\"\"Rectified Linear Unit (ReLU) function for a batch of samples\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        z: np.array[num_samples, num_neurons]\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "        h: np.array[num_samples, num_neurons]\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.where(v>0., v, 0.)\n",
    "\n",
    "#test\n",
    "z = np.array([[-1.,0.,1.,],[2.,-10.,3.,]])\n",
    "relu(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6533c4e-e9e1-4dad-9194-373576fef76e",
   "metadata": {},
   "source": [
    "<h1>Forward Pass of the Classifier Neural Network </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a1b2396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33810181, 0.66189819],\n",
       "       [0.48482283, 0.51517717],\n",
       "       [0.41493877, 0.58506123],\n",
       "       [0.45646297, 0.54353703],\n",
       "       [0.39578836, 0.60421164]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=================================================\n",
    "#HYPER PARAMETERS\n",
    "#=================================================\n",
    "input_dim = 3   #3 values in the input vector\n",
    "layer1_dim = 4  #4 neurons in hidden layer\n",
    "output_dim = 2  #2 categories for the classifier\n",
    "\n",
    "#example of a training batch\n",
    "\n",
    "X_train = np.array([[0,0,1], \n",
    "                    [0,1,0],\n",
    "                    [0,1,1],\n",
    "                    [1,0,0],\n",
    "                    [1,0,1]])\n",
    "\n",
    "Y_train = np.array([[0,1],\n",
    "                    [1,0],\n",
    "                    [0,1],\n",
    "                    [1,0],\n",
    "                    [0,1]])\n",
    "\n",
    "#create the network weights and biais (with random values)\n",
    "\n",
    "def init(seed=1):\n",
    "    \n",
    "    global b1, w1, b2, w2  #weights and biais for the hidden layer and the output layer\n",
    "    global g_b1, g_w1, g_b2, g_w2 #gradients\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    w1 = 2*np.random.random((input_dim , layer1_dim)) - 1\n",
    "    b1 = 2*np.random.random(layer1_dim) - 1\n",
    "    w2 = 2*np.random.random((layer1_dim, output_dim)) - 1\n",
    "    b2 = 2*np.random.random(output_dim) - 1\n",
    "\n",
    "    g_b2 = np.zeros_like(b2)\n",
    "    g_w2 = np.zeros_like(w2)\n",
    "    g_b1 = np.zeros_like(b1)\n",
    "    g_w1 = np.zeros_like(w1)\n",
    "\n",
    "    #w1 = 1.+np.array(range(0,input_dim*layer1_dim)).reshape(input_dim,layer1_dim)\n",
    "    #b1 = 1.+np.array(range(layer1_dim))\n",
    "    #w2 = 1.+np.array(range(0,layer1_dim*output_dim)).reshape(layer1_dim, output_dim)\n",
    "    #b2 = 1.+np.array(range(output_dim))\n",
    "\n",
    "#check that arrays shapes are all correct\n",
    "#h1=relu(np.matmul(X_train,w1) + b1)\n",
    "#h2=softmax(np.matmul(h1,w2)+b2)\n",
    "#print(h1)\n",
    "#print(h2)\n",
    "\n",
    "def forward(X):\n",
    "\n",
    "    '''forward pass of the neural network\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.array[num_samples, input_values]\n",
    "        batch of samples to be classified\n",
    "\n",
    "    global variables: \n",
    "        current state of the neural net,current weights and biais\n",
    "    \n",
    "        z1: np.array[num_samples, layer1_dim]\n",
    "            W1.X+B1\n",
    "            \n",
    "        h1: np.array[num_samples, layer1_dim]\n",
    "            Relu(z1)\n",
    "\n",
    "        z2: np.array[num_samples, layer2_dim]\n",
    "            W2.H1+B2\n",
    "            \n",
    "        h2: np.array[num_samples, layer2_dim]\n",
    "            Softmax(z2)\n",
    "\n",
    "        b1: np.array[layer1_dim]  \n",
    "            biais of the hidden layer\n",
    "            \n",
    "        w1: np.array[input_dim, layer1_dim]\n",
    "            weights of the hiddent layer\n",
    "            \n",
    "        b2: np.array[output_dim]\n",
    "            biais of the output layer\n",
    "            \n",
    "        w2: np.array[layer1_dim, output_dim]\n",
    "            weigths of the output layer\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    h2: np.array[num_samples, num_categories]\n",
    "        logits (probability vector corresponding to the categories of the classifier) for each sample of the batch\n",
    "    \n",
    "    '''\n",
    "\n",
    "    global b1, w1, b2, w2         #weights and biais\n",
    "    global g_b1, g_w1, g_b2, g_w2 #their gradients\n",
    "\n",
    "    global h1, z1   #first layer\n",
    "    global h2, z2   #output layer\n",
    "    \n",
    "    #forward \n",
    "    #matmul will return a matrix of size SxL1 where L1 is size of layer1\n",
    "    z1 = np.matmul(X,w1) + b1\n",
    "    h1 = relu(z1)  #shape=(S,L1)\n",
    "\n",
    "    z2 = np.matmul(h1, w2) + b2  #shape=(S,C) where C is size of output=number of categories\n",
    "    h2 = softmax(z2)  #shape=(S,C)\n",
    "    \n",
    "    return h2\n",
    " \n",
    "#test\n",
    "init()\n",
    "forward(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9510dbf-067b-4be7-bce9-4bcb9b822850",
   "metadata": {},
   "source": [
    "<h1>Training the network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dafe806b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5933506629954401,\n",
       " array([[0.01897769, 0.98102231],\n",
       "        [0.76351307, 0.23648693],\n",
       "        [0.04528619, 0.95471381],\n",
       "        [0.82548816, 0.17451184],\n",
       "        [0.06409521, 0.93590479]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def backrop_weights(X,Y,alpha):\n",
    "    \n",
    "    \"\"\"Backpropagation to calculate the gradient of the loss with respect to the weights and biais\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    X: np.array[num_samples, num_inputs]\n",
    "    \n",
    "    Y: np.array[num_sample, num_categories]\n",
    "        truth values as one-hot encoded vector over the corresponding categories, for each sample\n",
    "    \n",
    "    alpha: float\n",
    "        learning rate\n",
    "        \n",
    "    global variables: current state of the neural net,current weights and biais\n",
    "    \n",
    "        z1: np.array[num_samples, layer1_dim]\n",
    "            W1.X+B1\n",
    "            \n",
    "        h1: np.array[num_samples, layer1_dim]\n",
    "            Relu(z1)\n",
    "\n",
    "        z2: np.array[num_samples, layer2_dim]\n",
    "            W2.H1+B2\n",
    "            \n",
    "        h2: np.array[num_samples, layer2_dim]\n",
    "            Softmax(z2)\n",
    "\n",
    "        b1: np.array[layer1_dim]  \n",
    "            biais of the hidden layer\n",
    "            \n",
    "        w1: np.array[input_dim, layer1_dim]\n",
    "            weights of the hiddent layer\n",
    "            \n",
    "        b2: np.array[output_dim]\n",
    "            biais of the output layer\n",
    "            \n",
    "        w2: np.array[layer1_dim, output_dim]\n",
    "            weigths of the output layer\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    g_b1: np.array[layer1_dim]\n",
    "        dLoss/db^1_k\n",
    "        \n",
    "    g_w1: np.array[input_dim, layer1_dim]\n",
    "        dLoss/db^1_jk\n",
    "        \n",
    "    g_b2: np.array[output_dim]\n",
    "        dLoss/db^2_l\n",
    "        \n",
    "    g_w2: np.array[layer1_dim, output_dim]\n",
    "        dLoss/db^2_kl\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    \n",
    "    Backpropagation gradients calculated by hand...\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    global b1, w1, b2, w2\n",
    "    global g_b1, g_w1, g_b2, g_w2\n",
    "\n",
    "    global z1, h1\n",
    "    global z2, h2\n",
    "    \n",
    "    g_b2 = np.zeros_like(b2)\n",
    "    g_w2 = np.zeros_like(w2)\n",
    "    g_b1 = np.zeros_like(b1)\n",
    "    g_w1 = np.zeros_like(w1)\n",
    "    \n",
    "    g_loss_softmax = -(Y - h2)  #shape=S,C\n",
    "\n",
    "    g_b2 = np.sum(g_loss_softmax, axis=0) #shape C; sum(axis=0) is sum over samples\n",
    "\n",
    "    g_w2 = np.einsum(\"ij,ik\", h1, g_loss_softmax)\n",
    "    \n",
    "    dRelu = np.where(z1>0., 1., 0.)  #shape=S,L1\n",
    "\n",
    "    g_b1 += np.einsum(\"il,kl,ik->k\", g_loss_softmax, w2, dRelu)\n",
    "\n",
    "    g_w1 += np.einsum(\"il,kl,ik,ij->jk\", g_loss_softmax, w2, dRelu, X)\n",
    "    \n",
    "    b2 -= alpha * g_b2 / X.shape[0] #average over number of samples\n",
    "    w2 -= alpha * g_w2 / X.shape[0]\n",
    "    b1 -= alpha * g_b1 / X.shape[0]\n",
    "    w1 -= alpha * g_w1 / X.shape[0]\n",
    "    \n",
    "    return g_b1, g_w1, g_b2, g_w2\n",
    "    \n",
    "def train(X, Y, num_periods, alpha=0.1):\n",
    "    \n",
    "    global b1, w1, b2, w2\n",
    "    global g_b1, g_w1, g_b2, g_w2\n",
    "    \n",
    "    for p in range(num_periods):\n",
    "        \n",
    "        batch_X = X #[0:1,:]\n",
    "        batch_Y = Y #[0:1,:]\n",
    "        #print(\"batch_X:\",batch_X)\n",
    "        \n",
    "        pred = forward(batch_X)\n",
    "        loss, _ = cross_entropy_loss(batch_Y, pred)  #loss scalar, _ is vector of size S, total loss for entire batch, loss for each sample of batch\n",
    "        #print(p,loss)\n",
    "\n",
    "        #adjust the weights to reduce the loss\n",
    "        backrop_weights(batch_X, batch_Y, alpha)\n",
    "\n",
    "        '''\n",
    "        For testing, calculate the forward derivative of the loss with respect to the weights and biais through perturbations\n",
    "        eps=0.001\n",
    "        fg_b2 = np.zeros_like(b2)\n",
    "        for i in range(b2.shape[0]):\n",
    "            b2[i] += eps\n",
    "            loss1, _ = cross_entropy_loss(batch_Y, forward(batch_X))\n",
    "            fg_b2[i] = (loss1-loss)/eps\n",
    "            b2[i] -= eps\n",
    "        forward(X)\n",
    "        print(\"fg_b2\", fg_b2)\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        eps=0.001\n",
    "        fg_b1 = np.zeros_like(b1)\n",
    "        for i in range(b1.shape[0]):\n",
    "            b1[i] += eps\n",
    "            loss1, _ = cross_entropy_loss(batch_Y, forward(batch_X))\n",
    "            fg_b1[i] = (loss1-loss)/eps\n",
    "            b1[i] -= eps\n",
    "        forward(X)\n",
    "        print(\"fg_w2\", fg_w2)\n",
    "        '''\n",
    "        '''\n",
    "        eps=0.001\n",
    "        fg_w2 = np.zeros_like(w2)\n",
    "        for i in range(w2.shape[0]):\n",
    "            for j in range(w2.shape[1]):\n",
    "                w2[i][j] += eps\n",
    "                loss1, _ = cross_entropy_loss(batch_Y, forward(batch_X))\n",
    "                fg_w2[i][j] = (loss1-loss)/eps\n",
    "                w2[i][j] -= eps\n",
    "        forward(X)\n",
    "        print(\"fg_w2\", fg_w2)\n",
    "        '''\n",
    "        '''\n",
    "        eps=0.001\n",
    "        fg_w1 = np.zeros_like(w1)\n",
    "        for i in range(w1.shape[0]):\n",
    "            for j in range(w1.shape[1]):\n",
    "                w1[i][j] += eps\n",
    "                loss1, _ = cross_entropy_loss(batch_Y, forward(batch_X))\n",
    "                fg_w1[i][j] = (loss1-loss)/eps\n",
    "                w1[i][j] -= eps\n",
    "        forward(batch_X)\n",
    "       ''' \n",
    "        \n",
    "    return loss, pred\n",
    "\n",
    "init()\n",
    "train(X_train, Y_train, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a9ea0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ce1feee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "input_dim = 3\n",
    "layer1_dim = 5\n",
    "output_dim = 2  #2 classes\n",
    "\n",
    "S=100 #number of samples\n",
    "\n",
    "#generate a batch of uniformly random points in 3d space [-1,1]x[-1,1]x[-1,1]\n",
    "X1 = 2*np.random.rand(S,3) - 1\n",
    "\n",
    "#use plane x+y+z=0 to separate them in two classes by t\n",
    "Y1 = np.zeros((S,2))\n",
    "Y1[:,0] = np.where(X1[:,0]+X1[:,1]+X1[:,2]>0,1,0)\n",
    "Y1[:,1] = np.where(X1[:,0]+X1[:,1]+X1[:,2]>0,0,1)\n",
    "\n",
    "init(1)\n",
    "train(X1,Y1,100, alpha=0.1)\n",
    "\n",
    "pred = forward(X1)\n",
    "\n",
    "marker = np.where(pred[:,0]>=0.5,\"o\",\"s\")\n",
    "color = np.where(Y1[:,0]>0,\"green\",\"blue\")\n",
    "\n",
    "color = np.where((Y1[:,0]>0) & (pred[:,0]<0.5) | (Y1[:,1]>0) & (pred[:,1]<0.5),\"red\",color)\n",
    "\n",
    "#plot the results\n",
    "#marker are predicted values; colors are true values with red showing wrong classification\n",
    "fig = plt.figure()\n",
    "#ax = fig.add_subplot(projection='3d')\n",
    "ax = fig.add_subplot()\n",
    "for i in range(X1.shape[0]):\n",
    "    ax.scatter(X1[i,0],X1[i,1],c=color[i],marker=marker[i],alpha=0.5)\n",
    "    ax.scatter(X1[i,0],X1[i,1],c=color[i],marker=marker[i],alpha=0.5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abab007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ea25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69aef4c-190f-4aa3-9677-0cc6bdaa27da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
