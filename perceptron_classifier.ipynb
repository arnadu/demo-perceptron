{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "321512ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb60d3",
   "metadata": {},
   "source": [
    "Toy implementation of a one-hidden layer classifier <br>\n",
    "loss function = cross-entropy<br>\n",
    "output layer = softmax<br>\n",
    "hidden layer = relu<br>\n",
    "\n",
    "\n",
    "Cross entropy loss function for classification of a batch of several samples :\n",
    "\n",
    "$ loss = \\sum\\limits _{i=0}^{Samples-1} \\sum\\limits_{l=0}^{Categories-1} y_{il} . log(prediction_{il})  $\n",
    "\n",
    "$Samples$: the number of samples in the batch <br>\n",
    "$Categories$ : the number of categories <br>\n",
    "$y[i,l]$ the truth value for the i-th sample, as a one-hot encoded vector over the possible categories<br>\n",
    "$prediction[i,l]$ the predicted categories, a vector of probabilities ]0,1] for the i-th sample <br>\n",
    "\n",
    "$ \\frac {dLoss}{dpred_j} = $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "1c14ba29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.513306124309698, array([0.10536052, 2.30258509, 0.10536052]))"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CROSS ENTROPY LOSS FUNCTION\n",
    "\n",
    "#prepare a batch of 3 samples with 2 categories\n",
    "#----------------------------\n",
    "\n",
    "y = np.array([[0,1], [1,0], [1,0]])   #one-hot encoded category of each sample\n",
    "pred = np.array([[0.1,0.9], [0.1,0.9], [0.9,0.1]])  #predicted proba of each category for each sample\n",
    "\n",
    "\n",
    "#calculate cross entropy loss for a batch of samples\n",
    "#---------------------------------------------------\n",
    "def cross_entropy_loss(y, pred):\n",
    "    l = -np.log(pred)*y  #calculate y*log(p) for each category of each sample of the batch\n",
    "    individual_losses = np.sum(l, -1)  #sum over categories\n",
    "    batch_loss = np.sum(individual_losses) #sum over samples\n",
    "    return batch_loss, individual_losses\n",
    "\n",
    "#test\n",
    "#------\n",
    "cross_entropy_loss(y, pred)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310f87ad",
   "metadata": {},
   "source": [
    "SOFTMAX function as output layer\n",
    "\n",
    "$ softmax(z_i) =  \\frac{e^{z_i}}{\\sum\\limits_{k=1}^{Categories}e^{z_k}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "02f1f4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.53978687e-05, 9.99954602e-01],\n",
       "       [5.00000000e-01, 5.00000000e-01],\n",
       "       [9.93307149e-01, 6.69285092e-03]])"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare a batch of 3 samples with 2 categories\n",
    "#-----------------------------------------------\n",
    "\n",
    "\n",
    "z = np.array([[0, 10.], [0.,0.], [0, -5]])  #predicted proba of each category for each sample\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    \n",
    "    #input\n",
    "    # z a S*C matrix rows = samples, columns same dimension as number of categories\n",
    "    m = np.max(z)   #normalize to avoid large numbers \n",
    "    e = np.exp(z - m)\n",
    "    return e / e.sum(axis=1, keepdims=True) #[:,None]\n",
    "    \n",
    "   \n",
    "softmax(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "1207af67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.])"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def relu(v):\n",
    "    return np.where(v>0., v, 0.)\n",
    "\n",
    "h = np.array([-1.,0.,1.,])\n",
    "relu(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "9a1b2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "layer1_dim = 4\n",
    "output_dim = 2  #2 classes\n",
    "\n",
    "X_train = np.array([[0,0,1], \n",
    "                    [0,1,0],\n",
    "                    [0,1,1],\n",
    "                    [1,0,0],\n",
    "                    [1,0,1]])\n",
    "\n",
    "Y_train = np.array([[0,1],\n",
    "                    [1,0],\n",
    "                    [0,1],\n",
    "                    [1,0],\n",
    "                    [0,1]])\n",
    "\n",
    "def init(seed=1):\n",
    "    \n",
    "    global b1, w1, b2, w2\n",
    "    global g_b1, g_w1, g_b2, g_w2\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    w1 = 2*np.random.random((input_dim , layer1_dim)) - 1\n",
    "    b1 = 2*np.random.random(layer1_dim) - 1\n",
    "    w2 = 2*np.random.random((layer1_dim, output_dim)) - 1\n",
    "    b2 = 2*np.random.random(output_dim) - 1\n",
    "\n",
    "    g_b2 = np.zeros_like(b2)\n",
    "    g_w2 = np.zeros_like(w2)\n",
    "    g_b1 = np.zeros_like(b1)\n",
    "    g_w1 = np.zeros_like(w1)\n",
    "\n",
    "    #w1 = 1.+np.array(range(0,input_dim*layer1_dim)).reshape(input_dim,layer1_dim)\n",
    "    #b1 = 1.+np.array(range(layer1_dim))\n",
    "    #w2 = 1.+np.array(range(0,layer1_dim*output_dim)).reshape(layer1_dim, output_dim)\n",
    "    #b2 = 1.+np.array(range(output_dim))\n",
    "\n",
    "#h1=relu(np.matmul(X_train,w1) + b1)\n",
    "#softmax(np.matmul(h1,w2)+b2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "dafe806b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0576843351159573,\n",
       " array([[7.95516484e-04, 9.99204484e-01],\n",
       "        [9.66141220e-01, 3.38587805e-02],\n",
       "        [2.72356743e-03, 9.97276433e-01],\n",
       "        [9.87930191e-01, 1.20698094e-02],\n",
       "        [7.54411174e-03, 9.92455888e-01]]))"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def forward(X):\n",
    "\n",
    "    '''\n",
    "    X is expected to be shape=(S,I) \n",
    "    S: is the number of samples in a batch\n",
    "    I: the number of input values for each sample\n",
    "    '''\n",
    "\n",
    "    global b1, w1, b2, w2\n",
    "    global g_b1, g_w1, g_b2, g_w2\n",
    "\n",
    "    global layer1_v, layer1_o\n",
    "    global layer2_v, layer2_o\n",
    "    \n",
    "    #forward \n",
    "    #matmul will return a matrix of size SxL1 where L1 is size of layer1\n",
    "    layer1_v = np.matmul(X,w1) + b1\n",
    "    layer1_o = relu(layer1_v)  #shape=(S,L1)\n",
    "\n",
    "    layer2_v = np.matmul(layer1_o,w2) + b2  #shape=(S,C) where C is size of output=number of categories\n",
    "    layer2_o = softmax(layer2_v)  #shape=(S,C)\n",
    "    \n",
    "    return layer2_o\n",
    "\n",
    "\n",
    "def backrop_weights(X,Y):\n",
    "\n",
    "    global b1, w1, b2, w2\n",
    "    global g_b1, g_w1, g_b2, g_w2\n",
    "\n",
    "    global layer1_v, layer1_o\n",
    "    global layer2_v, layer2_o\n",
    "    \n",
    "    g_b2 = np.zeros_like(b2)\n",
    "    g_w2 = np.zeros_like(w2)\n",
    "    g_b1 = np.zeros_like(b1)\n",
    "    g_w1 = np.zeros_like(w1)\n",
    "    \n",
    "    g_loss_softmax = -(Y-layer2_o)  #shape=S,C\n",
    "\n",
    "    g_b2 = np.sum(g_loss_softmax, axis=0) #shape C; sum(axis=0) is sum over samples\n",
    "\n",
    "    g_w2 = np.einsum(\"ij,ik\",layer1_o, g_loss_softmax)\n",
    "    \n",
    "    dRelu = np.where(layer1_v>0., 1., 0.)  #shape=S,L1\n",
    "\n",
    "    g_b1 += np.einsum(\"il,kl,ik->k\",g_loss_softmax,w2,dRelu)\n",
    "\n",
    "    g_w1 += np.einsum(\"il,kl,ik,ij->jk\",g_loss_softmax,w2,dRelu,X)\n",
    "    \n",
    "    b2 -= alpha * g_b2 / X.shape[0] #average over number of samples\n",
    "    w2 -= alpha * g_w2 / X.shape[0]\n",
    "    b1 -= alpha * g_b1 / X.shape[0]\n",
    "    w1 -= alpha * g_w1 / X.shape[0]\n",
    "    \n",
    "    return g_b1, g_w1, g_b2, g_w2\n",
    "    \n",
    "def train(X, Y, num_periods, alpha=0.1):\n",
    "    \n",
    "    global b1, w1, b2, w2\n",
    "    global g_b1, g_w1, g_b2, g_w2\n",
    "    \n",
    "    for p in range(num_periods):\n",
    "        \n",
    "        batch_X = X #[0:1,:]\n",
    "        batch_Y = Y #[0:1,:]\n",
    "        #print(\"batch_X:\",batch_X)\n",
    "        \n",
    "        pred = forward(batch_X)\n",
    "        loss, _ = cross_entropy_loss(batch_Y, pred)  #loss scalar, _ is vector of size S, total loss for entire batch, loss for each sample of batch\n",
    "        #print(p,loss)\n",
    "\n",
    "        '''\n",
    "        eps=0.001\n",
    "        fg_b2 = np.zeros_like(b2)\n",
    "        for i in range(b2.shape[0]):\n",
    "            b2[i] += eps\n",
    "            loss1, _ = cross_entropy_loss(batch_Y, forward(batch_X))\n",
    "            fg_b2[i] = (loss1-loss)/eps\n",
    "            b2[i] -= eps\n",
    "        forward(X)\n",
    "\n",
    "        eps=0.001\n",
    "        fg_b1 = np.zeros_like(b1)\n",
    "        for i in range(b1.shape[0]):\n",
    "            b1[i] += eps\n",
    "            loss1, _ = cross_entropy_loss(batch_Y, forward(batch_X))\n",
    "            fg_b1[i] = (loss1-loss)/eps\n",
    "            b1[i] -= eps\n",
    "        forward(X)\n",
    "\n",
    "        eps=0.001\n",
    "        fg_w2 = np.zeros_like(w2)\n",
    "        for i in range(w2.shape[0]):\n",
    "            for j in range(w2.shape[1]):\n",
    "                w2[i][j] += eps\n",
    "                loss1, _ = cross_entropy_loss(batch_Y, forward(batch_X))\n",
    "                fg_w2[i][j] = (loss1-loss)/eps\n",
    "                w2[i][j] -= eps\n",
    "        forward(X)\n",
    "        \n",
    "        eps=0.001\n",
    "        fg_w1 = np.zeros_like(w1)\n",
    "        for i in range(w1.shape[0]):\n",
    "            for j in range(w1.shape[1]):\n",
    "                w1[i][j] += eps\n",
    "                loss1, _ = cross_entropy_loss(batch_Y, forward(batch_X))\n",
    "                fg_w1[i][j] = (loss1-loss)/eps\n",
    "                w1[i][j] -= eps\n",
    "        forward(batch_X)\n",
    "       ''' \n",
    "        \n",
    "        backrop_weights(batch_X, batch_Y)\n",
    "#        print(fg_b1, g_b1)\n",
    "#        print(fg_b2, g_b2)\n",
    "#        print(fg_w1, g_w1)\n",
    "#        print(fg_w2,g_w2)\n",
    "\n",
    "    \n",
    "    return loss, pred\n",
    "\n",
    "init()\n",
    "train(X_train, Y_train, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a9ea0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "8ce1feee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD4CAYAAADo30HgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAgBklEQVR4nO3dfawdd33n8ffHJiYk4RKCTWIcuw6KlW5aAZteBVNauAiCEm+pS2WqZCtIl3Td7HKldtUtuCWKUNGqWfeBFesUatJoQ9WSRoYUixpCSOtNC+uub6w8m2DHOOTaJnZCEpMFgY2/+8fMrecen8c7c+bMzPm8pKNzzjyc87tn5v6+M79HRQRmZmZzFo06AWZmVi0ODGZmNo8Dg5mZzePAYGZm8zgwmJnZPC8bdQIWYunSpbF69epRJ8PMrFYeeOCBZyNiWa/tahkYVq9ezczMzKiTYWZWK5Ke6mc7FyWZmdk8DgxmZjaPA4OZmc1TSGCQdLuko5Ie7bBekj4pab+khyVdkVl3taQn0nWbikiPmZktXFF3DP8LuLrL+muANeljI/ApAEmLgVvT9ZcD10m6vKA0mZnZAhQSGCLifuB7XTZZD3w2EruA8yUtB64E9kfEgYj4MXBnuq011bZtMDUFa9Ykz9u2jTpFZtairDqGFcDTmfez6bJOy88gaaOkGUkzx44dG1pCbYi2bYMPfxheeAGWL0+eP/xhBweziikrMKjNsuiy/MyFEVsjYjIiJpct69k/w6poyxaYmIDzz4dFi5LniYlkuZlVRlmBYRZYmXl/MXC4y3JrokOHkkCQNTGRLDezyigrMGwHPpC2TloLvBgRR4DdwBpJl0haAlybbmtNtGIFHD8+f9nx48lyM6uMopqrfg74P8BlkmYl3SDpRkk3ppvsAA4A+4HPAP8ZICJOAtPAPcBe4K6IeKyINFkFTU8ngeCFF+DUqeT5+PFk+Si4ItysrULGSoqI63qsD+BDHdbtIAkc1nQbNiTPW7YkxUcrVsBNN51eXqa5ivCJifkV4dl0mo0p1XHO58nJyfAgepbL1FQSDM4///Syufc7d44iRWZDJ+mBiJjstZ2HxLDx5Ipws44cGGw8uSLcrCMHBqueMiqFq1YRblYhtZyoxxqsrErhKlWEm1WMK5+tWlwpbHW3bdv8C47p6cpccLjy2eppHCqF3X+iuRoyHpgDg1VL0yuFm5BxDDuw1TlwNmQ8MAcGq5amVwrXPeMYdmCre+BsyB2vA4NVy4YNsHlzkmEeOZI8b95cmTLa3OqecQw7sNU9cDbkjtetkqx6NmxoTiBotWLFmZXrdco4Dh1KruSzigxsw/78YZuePt2KbmIiObbHjyct3mrEdwx2Wp3Lduui7kVlw74irvsVd0PueB0YLFH3st26qHvGMezAVvfACcmx3LkT9u1LnutybDPcj8ES7j9g/Rp2O/0K9wOou377MTgwWGLNmuROYVHmJvLUqeSqdt++0aWrqZz52Qi4g5sNpu5lu3XiYjuruKJmcLta0hOS9kva1Gb970l6MH08Kuknki5I1x2U9Ei6zrcBo9KEst26qHuTTGu83M1VJS0GbgWuAmaB3ZK2R8Tjc9tExB8Df5xu/x7gv0TE9zIf846IeDZvWiwHDypXnro3ybTGK6Ifw5XA/og4ACDpTmA98HiH7a8DPlfA91rRmtx/oErq3pehE9ebNEYRRUkrgKcz72fTZWeQdA5wNfD5zOIAvirpAUkbO32JpI2SZiTNHDt2rIBkW6PUqQ9GE4vtXG/SKEUEBrVZ1qmp03uAr7cUI701Iq4ArgE+JOlt7XaMiK0RMRkRk8uWLcuXYutfHTLcumVKde/L0E5R9SZ1ON/GQBFFSbPAysz7i4HDHba9lpZipIg4nD4flXQ3SdHU/QWky/Iqa9KcvLKZEpx+3rKlWunMalqxXRH1JnU538ZAEXcMu4E1ki6RtIQk89/eupGkVwFvB76YWXaupFfOvQbeDTxaQJqsCHVpPVP3gemaoIjmznU438q+oxnRHVTuwBARJ4Fp4B5gL3BXRDwm6UZJN2Y2fS/w1Yj4f5llFwL/LOkh4P8Cfx8RX8mbJitIXTJc98EYvSLqTap+vpVdZDnCIlL3fLbO6jJMRrYIIjuiZd3L7esmb6ukqp9vZadvCN/nns+WX11azzSxMreO8g4eV/Xzrew7mhHeQXk+BuusTp3emlaZm1cd+xRU/Xwru//JCPu7ODBYd85w66fOrXuqfL6VPQnPCCf9cVGSWdPUoXVPHZVdZDnCIlJXPjdFHYsObDg8hLp14MrncVK3nr82XG6+azk5MDSBiw4sq+qte6zyHBiaoOodgwbl8XLycfNdy8mtkpqgScM417lFTZVUuXWPVZ7vGJqgSUUHLhYzGzkHhiZoUtFB04rFzGrIRUlN0ZSigyYVi5nVlO8YrFqaVCzWypXqVhO+Y7Bqqfp4OQvlSnWrEfd8NitD1YeUtrHgns9mVeJKdasRBwazMniYCquRQgKDpKslPSFpv6RNbdZPSXpR0oPp4+Z+9x0qVwZWV9OOTZMr1a1xclc+S1oM3ApcBcwCuyVtj4jHWzb9p4j4pQXuWzxXBlZXE49NUyvVrZGKuGO4EtgfEQci4sfAncD6EvbNxz1sq6upxybv1JdmJSkiMKwAns68n02XtXqLpIckfVnSzwy4L5I2SpqRNHPs2LH8qc5bGdi0oo4qcUWt2UgVERjUZllrG9g9wE9FxBuB/wn83QD7JgsjtkbEZERMLlu2bKFpPS1PZaDnPxguV9SajVQRgWEWWJl5fzFwOLtBRByPiJfS1zuAsyQt7WffoclTGdjUoo6qcEWt2UgVERh2A2skXSJpCXAtsD27gaSLJCl9fWX6vc/1s+/Q5Bl4zkUdw9WkQQHNaih3q6SIOClpGrgHWAzcHhGPSboxXf9pYAPwnySdBH4IXBtJl+u2++ZNU98WOvBcQwZ6W7sWnn32zOVLl8KuXeWnZ56mDApoVkOFjJWUFg/taFn26czrLUDbcpZ2+1be9PTp5pMTE0lQOH48aX5YI88+CxdffOby2dny02Jm1eFB9BbCbdLNrMEcGBbKRR1m1lAeK8nMzOZxYDAzK0tNOsa6KGmMLV3avqJ56dIhfum2bfPrZqanXSRn46FGY4CNbWCodFPNkpT+d9boH8OscNmOsXD6ecuWyp3/YxsY3FRzBGr0j2FWuEOHkguirIp2jHUdg5XHPcZtnNVoDDAHBitPjf4xSlWTCknrQ7djWaMxwBwYrDw1+scojUfqbY5ex7JGY4ApGbKoXiYnJ2NmZibXZ1x6aec6hv37c320deNWSfNNTZ057tbc+507R5EiW6gaHEtJD0TEZK/txrbyeSRNNc09xlvVqELSemjQsRzbwDAuTVKt4hoyUq/RqGPpOgazUXK9S3M06FiO7R2DWSV4pN7maNCxHNvKZ6sn91g3W7h+K58LKUqSdLWkJyTtl7Spzfpfl/Rw+viGpDdm1h2U9IikByU5t7eu5nqstz7aBQuzoejV76QB/VJyBwZJi4FbgWuAy4HrJF3estm3gbdHxBuAjwNbW9a/IyLe1E8kM6u0BmQK1kWvvgoN6ZdSRB3DlcD+iDgAIOlOYD3w+NwGEfGNzPa7gDY9CIbHxQ99anAfg1LOAQ8S2Hy9xvtqyHhgRQSGFcDTmfezwJu7bH8D8OXM+wC+KimAv4iI1rsJACRtBDYCrFq1aqAEesC8PjQ8UyvlHGhIpmBd9Oqr0JC+DEXUMajNsrY12pLeQRIYPpJZ/NaIuIKkKOpDkt7Wbt+I2BoRkxExuWzZsrxptlbZTG3RouR5YiJZbt3NFR99/evw7W/D0aOn19UwU7Aueo331ZDxwIoIDLPAysz7i4HDrRtJegNwG7A+Ip6bWx4Rh9Pno8DdJEVTVraajHw612O99TGyHuvZMuVXvAJ+9CM4cOB0cKhhpmBd9Oqr0JC+DEUUJe0G1ki6BDgEXAv8++wGklYBXwDeHxHfyiw/F1gUEd9PX78b+MMC0tTVnj1w4kTyuPTS08vHus6hJr02K3d8sndaK1fCk08myw8dgiVLkt/wpptGmkQrUK++Cg3py5A7METESUnTwD3AYuD2iHhM0o3p+k8DNwOvAf5cEsDJtAXShcDd6bKXAX8TEV/Jm6ZeTpyAl788eZ0tdx7rOofp6dN1ChMTSYbmTK23bJnyXBHn00/DD36QBIsaZgrWQ6/xvhowHlghPZ8jYgewo2XZpzOvfxP4zTb7HQDe2Lq8aK0D5p04kTyfddawv7lGGnKl08nQBk1svdNatiw5sSo0oqZVWEVbAo7FkBitxQ+dhtweew240ulkaEVQvtOyhapwS8CxCAxmgxioz0PD77RsiCrcvNmBwazFwH0eGnyn1U0lO45WtGimrQr3eRjLwOBJeszyq1zH0QoXzbRV4ZaAYzkfw65dyfSdrY/KNYU0s/7VrZNmP30eRjT21ljeMZhZA1W4aKatbP3U3r1J58hXvGJ+IBvRHdBY3jFYgzRlNNOi/46m/C6DqONwFBs2JHcI554Lq1cnx2suANx888jugBwYrL6GNMRx67Abl+/dxi27pviH2SFlskX/HQ0Z+nlgdR2OolMR2Le/PbJhajyDm9XX1NSZlXdz74vqXJat0Mz2U9i8ubjb+aL/jjJ+F9wqqTBr1iQBfFHmOv3UKdi9Gy67rNDj2O8Mbq5jsPoqo0y5jLbmRf8dJZW1V7KxRh2bDndqnfT6158uGiu586SLkqy+yihTLmPU2aL/jjqWtY+z1iKwgwfhm99MbsfOPjsZw+fIkSRwFHmn2oUDg9VXGWXKZWSyRf8ddS1rH1cbNiQZ/vnnw7598N3vwmtfmxQjLVkCP/wh/NEfJcVHJd0NOTBYfWX/oYZ1RVVGJlv031HG72LF2rAhyfgvuwx++qeTYqQR9sVw5fOQVbKCzgZTxwpNq6dOFdFHjiR3Ezm58rkiKjdsgA2ujhWaVk8VGSbDRUlmZlVRkfqhQgKDpKslPSFpv6RNbdZL0ifT9Q9LuqLffc3MxkZF6odyFyVJWgzcClwFzAK7JW2PiMczm10DrEkfbwY+Bby5z31tQK7XMKuxChRdFlHHcCWwP52mE0l3AuuBbOa+HvhsJDXduySdL2k5sLqPfW1Aw6rX6BZw5r633ToHI7N6KSIwrACezryfJbkr6LXNij73BUDSRmAjwKpVq/KluERNmvuhV8BxJbtZMxQRGNRmWWsb2E7b9LNvsjBiK7AVkuaqgyRwlHy1bGZ1U0RgmAVWZt5fDBzuc5slfexrZmYlKqJV0m5gjaRLJC0BrgW2t2yzHfhA2jppLfBiRBzpc18zMytR7juGiDgpaRq4B1gM3B4Rj0m6MV3/aWAHsA7YD/wA+A/d9s2bpnHXpHoNMyufh8SwvrlVklm9eUgMK5wzeLPx4CExzKw/4ziP9JjyHUMJ3BPZai87xWl2HmkYeS9dK54DQwk8wqrVXhlTnFpluCjJzHorY4pTqwwHBjPrzfNIjxUHBrNxstAK5IrME2DlcB2D2bjIU4E8tz47xelNN7l+oaHcwa0EbpVklTA1dea0kXPvd+4cRYqsZO7gViHO/K0SDh1K7hSyCqhA9oVP8zgwmI2LIU007+bYzePAYI3nK9rU9PTpOoWJiSQoHD+e1BWYZTgwWOP5ijblCmTrkwOD2TipwETzY2XbtvmBeHq6Fr+/+zGYmQfIG4a55sEvvDC/eXANflvfMZiNu5wD5HliqA5qPL6UA4PZuMuZgY1VBf4ghtQ8uAy5ipIkXSDpXkn70udXt9lmpaR/lLRX0mOSfjuz7mOSDkl6MH2sy5Mes3bmrmhbH2N/RTvHA+QNR9HjS5VY3Jf3jmETcF9E3CJpU/r+Iy3bnAR+NyL2SHol8ICkeyPi8XT9JyLiT3Kmw6wjX9H2MKT+DWOvyObBJc+HkbfyeT1wR/r6DuBXWjeIiCMRsSd9/X1gL+AzzqwqPEDecGzYAJs3JwH3yJHkefPmhWXk2eK+RYuS54mJZPkQ5BorSdILEXF+5v3zEXFGcVJm/WrgfuBnI+K4pI8BvwEcB2ZI7iye77DvRmAjwKpVq37uqaeeWnC6zaxFTZtVjo01a5I7hUWZa/lTp5KAs29f3x/T71hJPQODpK8BF7VZ9VHgjn4Dg6TzgP8N/LeI+EK67ELgWSCAjwPLI+KDvRJdt0H0zMxyKWgAxMIG0YuId3X5kmckLY+II5KWA0c7bHcW8Hngr+eCQvrZz2S2+QzwpV7pMTMbOyUPZ5K3jmE7cH36+nrgi60bSBLwl8DeiPizlnXZtlzvBR7NmR6zoTjvPHjZy858nHfeqFNmY6HI+oo+5K1jeA1wF7AK+A7wvoj4nqTXAbdFxDpJvwD8E/AIcCrd9Q8iYoekvwLeRFKUdBD4rYg40ut7XZRkZesUBF56CU6eLD89ZgtRynwMEfEc8M42yw8D69LX/wyow/7vz/P9ZmZWPI+VZGZm8zgwmJnZPA4MZmY2jwODWR/OPjupaG59nH32qFNmbXkY8VwcGMz6MNf6qPXx0kujTlnKGeFpNZ4HoSocGMzqzhnhfCWPK9REDgxmdeeMcD4PI56bA4NZ3TkjnK/oeRDGkAODWd05I5zPw4jn5sBghVq7Fi699MzH2rWjTlmDLSQjbHJldcnjCjWR53weobVr4dlnz1y+dGl9Zx179lm4+OIzl7ebLN4KMpfhZedTuOmmzhlhybOB9a3IOSE2bHAgyMGBYYSciVphBskIs5XVcPp5y5bRZaZVDVZjyoHBqqeJt1JVcuhQkvlmjbqyuorBaow5MFj1+FZquFasOHM2sC6V1aXE6SoGqzHmymezcTNgZfVcnG59tAsWC+aWVZXiOwYr1NKl7S/sly4tPy3WwaCV1WUoeerKYWpCSWiuwCDpAuBvgdUkM7D9WkQ832a7g8D3gZ8AJ+dmEOp3/6ZqYiZalxN/7FWt1U4Vg9UCNaEkNO8dwybgvoi4RdKm9P1HOmz7johojaOD7D80o4rwZWWiTbiCsTFQtWA1xvIGhvXAVPr6DmAng2XsefcvRBMifDe1+/uaeCtlViN5A8OFEXEEICKOSHpth+0C+KqkAP4iIrYOuD+SNgIbAVatWpUz2VZpvo2pFMfp8dMzMEj6GnBRm1UfHeB73hoRh9OM/15J34yI+wfYnzSYbAWYnJyMQfY1s4WrTZwusuf0mOsZGCLiXZ3WSXpG0vL0an85cLTDZxxOn49Kuhu4Ergf6Gt/M7OuKtRzugl3WHmLkrYD1wO3pM9fbN1A0rnAooj4fvr63cAf9ru/mVlPFeo5XZs7rC7yBoZbgLsk3QB8B3gfgKTXAbdFxDrgQuBuSXPf9zcR8ZVu+5etCRG+m6b/fWbuOV2sXIEhIp4D3tlm+WFgXfr6APDGQfYvWxMifDdN//vMBh3mw7rzkBhmVn+enKdQHhLDrAbcSbGHBvWcrgIHBrMaqEonxaEFqCKamrrndGEcGMysb0MJUEU3NXV/htxcx2Bmo5VtarpoUfI8MZEsH9RckHnhhflBpklzWpfAgcGshn7x6DY+8eAU9z61Bqam6p3xHTqUBIKshTY1LTLIjDEHBrOa+cWj27jxwIc578QLHF3UgKviIifpKTLIjDEHBrMamOukODsL/+7AFl48NcH3Tp3PWS9vwFVxkU1NPRNcIVz5bI3TxKad89K9Ju3lm72sK+mqeCi96ItsatqgmeBGyYHBGqcqTTuHZoS9fIcWWItqaur+DIVwYDCrG18Vd+f+DLk5MJjVja+KbcgcGMxq5HT9yYb0AczC0j+BXY4LVhAHBrMaaXz9iVWCA0OFDNKapoktb4rShPknOh3fp59uHxjMiuTAUCGDXA025cpxGAGuCYGx0/E9eLD0pNgYyhUYJF0A/C2wGjgI/FpEPN+yzWXpNnNeD9wcEf9D0seA/wgcS9f9QUTsyJOmcbJnD5w4MX/ZiRNJZluXzLEpAc6sSfL2fN4E3BcRa4D70vfzRMQTEfGmiHgT8HPAD4C7M5t8Ym69g8JgTpyAl798/uOss9pfgZuZ9StvUdJ6YCp9fQewE/hIl+3fCTwZEU/l/F6zsbR4cf3rT6z68gaGCyPiCEBEHJH02h7bXwt8rmXZtKQPADPA77YWRZnZaStXwv79OT7AcxVYH3oGBklfAy5qs+qjg3yRpCXALwO/n1n8KeDjQKTPfwp8sMP+G4GNAKtWrRrkq2tjkNY0S5e2r4g866zCk2UjMJSWVUVPiGONpYhY+M7SE8BUerewHNgZEZd12HY98KGIeHeH9auBL0XEz/b63snJyZiZmVlwupvi0ks7V9zmuqoskZvdlmhq6swxlube79yZ++N9LKtP0gMRMdlru7xFSduB64Fb0ucvdtn2OlqKkSQtnyuKAt4LPJozPWOlCe31nWGU6FA6KmtWgaOyNr2F2TgFvryB4RbgLkk3AN8B3gcg6XXAbRGxLn1/DnAV8Fst+2+W9CaSoqSDbdZbF007GW3IRjgqaxM0PfBl5QoMEfEcSUuj1uWHgXWZ9z8AXtNmu/fn+X4zG4BHZbU+ueez2bjwqKzWJwcGq6VxKu8tlOcqsD6MXWBwhtIM41TeWxdNaAxhibELDM5QzIajiAurKl+4jVPgG7vAYGZDUkCv6ipfuI06MJXJgcHM8nOv6kZxYDBrsNKKZrZsSYLCXB+JuectWxwYasiBwWppnMp78yitaGbIvaqtXGMXGKqSoVS5kq0O/BtVjHtVN8rYBYZhZiiDZPZVrmQzG1hBvaqrcuE27sYuMAyTM3sbWwX1qvadYDU4MJhZMdyrujEcGMwazEUzthAODCO0Zw+cODF/2YkTSV1F9pbaFdX1U5Vj5vPDFsKBYUTmpuZsnYrznHPOzFBcd1E/PmbVU5VgXQcODAUa5LZ9167uU3OaWbEcrPvnwFAgX3WYWRMsyrOzpPdJekzSKUkdJ5iWdLWkJyTtl7Qps/wCSfdK2pc+vzpPeszMLL9cgQF4FPhV4P5OG0haDNwKXANcDlwn6fJ09SbgvohYA9yXvjczsxHKO+fzXgBJ3Ta7EtgfEQfSbe8E1gOPp89T6XZ3ADuBj+RJU530WyfhJof142NmdVZGHcMK4OnM+1ngzenrCyPiCEBEHJH02k4fImkjsBFg1apVQ0pqufqtk3DdRf34mFWPg3X/egYGSV8DLmqz6qMR8cU+vqPd7UT0sd/8HSK2AlsBJicnB97fzMabg3X/egaGiHhXzu+YBVZm3l8MHE5fPyNpeXq3sBw4mvO7zMwsp7yVz/3YDayRdImkJcC1wPZ03Xbg+vT19UA/dyBmZjZEeZurvlfSLPAW4O8l3ZMuf52kHQARcRKYBu4B9gJ3RcRj6UfcAlwlaR9wVfrezMxGSBH1K66fnJyMmZmZUSfDzKxWJD0QER37nM0poyjJzMxqpJZ3DJKOAU8tYNelQJthtCrBaVuYKqcNqp0+p21h6py2n4qIZb0+pJaBYaEkzfRzGzUKTtvCVDltUO30OW0LMw5pc1GSmZnN48BgZmbzjFtg2DrqBHThtC1MldMG1U6f07YwjU/bWNUxmJlZb+N2x2BmZj04MJiZ2TyNCwxVnlWun8+WdJmkBzOP45J+J133MUmHMuvWlZm2dLuDkh5Jv39m0P2HlTZJKyX9o6S96fH/7cy6wn+3TudPZr0kfTJd/7CkK/rdt4S0/XqapoclfUPSGzPr2h7fEtM2JenFzLG6ud99S0jb72XS9aikn0i6IF037N/tdklHJT3aYX2x51tENOoB/BvgMpJJfyY7bLMYeBJ4PbAEeAi4PF23GdiUvt4E/PcC0zbQZ6fp/C5JpxSAjwH/dUi/W19pAw4CS/P+bUWnDVgOXJG+fiXwrcwxLfR363b+ZLZZB3yZZNj5tcC/9LtvCWn7eeDV6etr5tLW7fiWmLYp4EsL2XfYaWvZ/j3AP5Txu6Wf/zbgCuDRDusLPd8ad8cQEXsj4okem/3rrHIR8WNgblY50uc70td3AL9SYPIG/ex3Ak9GxEJ6eQ8q79890t8tIo5ExJ709fdJBmxcUWAasrqdP9k0fzYSu4DzlQwt38++Q01bRHwjIp5P3+4iGQq/DHn+9pH/bi2uAz5X4Pd3FRH3A9/rskmh51vjAkOf2s0qN5eJzJtVDug4q9wCDPrZ13LmyTed3ireXmRxzQBpC+Crkh5QMqveoPsPM20ASFoN/FvgXzKLi/zdup0/vbbpZ99hpy3rBpIrzTmdjm+ZaXuLpIckfVnSzwy477DThqRzgKuBz2cWD/N360eh51sZU3sWThWZVa7tB3dJ24CfswT4ZeD3M4s/BXycJK0fB/4U+GDJaXtrRBxWMg3rvZK+mV7N5FLg73YeyT/s70TE8XRxrt+t3de0WdZ6/nTaZmjnXo/vPXND6R0kgeEXMouHcnwHSNsekqLTl9K6oL8D1vS577DTNuc9wNcjInsFP8zfrR+Fnm+1DAxR4VnluqVN0iCffQ2wJyKeyXz2v76W9BngS2WnLSIOp89HJd1Ncqt6PxX43SSdRRIU/joivpD57Fy/Wxvdzp9e2yzpY99hpw1JbwBuA66JiOfmlnc5vqWkLRPMiYgdkv5c0tJ+9h122jLOuJMf8u/Wj0LPt3EtShrVrHKDfPYZZZhppjjnvUDbFgrDSpukcyW9cu418O5MGkb6u0kS8JfA3oj4s5Z1Rf9u3c6fbJo/kLYWWQu8mBaD9bPvUNMmaRXwBeD9EfGtzPJux7estF2UHkskXUmSRz3Xz77DTluaplcBbydzDpbwu/Wj2PNtWLXoo3qQ/OPPAj8CngHuSZe/DtiR2W4dScuVJ0mKoOaWvwa4D9iXPl9QYNrafnabtJ1D8s/wqpb9/wp4BHg4PbjLy0wbScuGh9LHY1X63UiKQyL9bR5MH+uG9bu1O3+AG4Eb09cCbk3XP0KmhVync6/A36tX2m4Dns/8TjO9jm+JaZtOv/shkorxn6/K75a+/w3gzpb9yvjdPgccAU6Q5G83DPN885AYZmY2z7gWJZmZWQcODGZmNo8Dg5mZzePAYGZm8zgwmJnZPA4MZmY2jwODmZnN8/8BVVkJ/UFxALYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "input_dim = 3\n",
    "layer1_dim = 5\n",
    "output_dim = 2  #2 classes\n",
    "\n",
    "S=100 #number of samples\n",
    "\n",
    "X1 = 2*np.random.rand(S,3) - 1\n",
    "\n",
    "Y1 = np.zeros((S,2))\n",
    "Y1[:,0] = np.where(X1[:,0]+X1[:,1]+X1[:,2]>0,1,0)\n",
    "Y1[:,1] = np.where(X1[:,0]+X1[:,1]+X1[:,2]>0,0,1)\n",
    "\n",
    "\n",
    "\n",
    "#plt.scatter(X1[:,0],X1[:,1],c=color,alpha=0.5)\n",
    "init(1)\n",
    "train(X1,Y1,100, alpha=0.1)\n",
    "\n",
    "pred = forward(X1)\n",
    "\n",
    "color = np.where(Y1[:,0]>0,\"red\",\"blue\")\n",
    "marker = np.where(pred[:,0]>0.5,\"o\",\"s\")\n",
    "\n",
    "fig = plt.figure()\n",
    "#ax = fig.add_subplot(projection='3d')\n",
    "ax = fig.add_subplot()\n",
    "for i in range(X1.shape[0]):\n",
    "    ax.scatter(X1[i,0],X1[i,1],c=color[i],marker=marker[i],alpha=0.5)\n",
    "    ax.scatter(X1[i,0],X1[i,1],c=color[i],marker=marker[i],alpha=0.5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abab007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ea25c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
