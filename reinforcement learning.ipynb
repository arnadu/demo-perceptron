{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c097b7e-7da2-4755-80bb-d353cf48e2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Tensorflow 2.0\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from ipycanvas import Canvas, RoughCanvas, hold_canvas\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        print(gpu)\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "#!jupyter labextension list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9daa94b0-6dc3-4ba3-b839-04b085aabaf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77b4da7a86f4cd7aa5f41975f575292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RoughCanvas(height=200, width=1000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Env0():\n",
    "    \n",
    "    def __init__(self, num_steps):\n",
    "        self.num_steps = num_steps\n",
    "        self.num_trees = 2\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.trees = [(random.randrange(100,300)), (random.randrange(400,600))]\n",
    "        self.fruits = [(90.,20),(105.,10)] #array of position, remaining lifetime\n",
    "        self.agent = random.randrange(0., 800.)\n",
    "        self.countfruits = 0\n",
    "        self.score = 0.\n",
    "        self.timestep = 0\n",
    "        return self.obs()\n",
    "\n",
    "    def obs(self):\n",
    "        o = [(xt - self.agent) for (xt) in self.trees]\n",
    "        return o\n",
    "    \n",
    "        \n",
    "    def step(self, action):\n",
    "\n",
    "        self.timestep += 1\n",
    "        \n",
    "        #remove old fruits\n",
    "        self.fruits = [(x,t-1) for (x,t) in self.fruits if t>0]\n",
    "\n",
    "        #add new ones\n",
    "        for (xt) in self.trees:\n",
    "            if random.random()>.9:\n",
    "                x = xt + random.randrange(-50,50,1)\n",
    "                t = random.randrange(50,100)\n",
    "                self.fruits.append((x,t))\n",
    "\n",
    "        #move the agent\n",
    "        direction = -1 if action==0 else +1\n",
    "        distance = direction * random.randint(1,5)\n",
    "        self.agent += distance  #change the position\n",
    "        \n",
    "        #calculate the reward\n",
    "        \n",
    "        self.score -= abs(distance)  #consume energy, negative reward, for moving\n",
    "        \n",
    "        found = [i for i in range(len(self.fruits)) if self.fruits[i][0]==self.agent]\n",
    "        self.score += 100 * len(found) #it is possible that several fruits have the same location\n",
    "        self.countfruits += len(found)\n",
    "        self.fruits = [f for i,f in enumerate(self.fruits) if i not in found]\n",
    "        \n",
    "        o = self.obs()\n",
    "        reward = self.score\n",
    "        terminated = True if self.timestep>=self.num_steps else False\n",
    "        if self.agent<0 or self.agent>1000:\n",
    "            terminated = True\n",
    "        \n",
    "        return o, reward, terminated   \n",
    "\n",
    "    \n",
    "    def init_canvas(self):\n",
    "        #canvas = Canvas(width=1000, height=200)\n",
    "        self.canvas = RoughCanvas(width=1000, height=200)\n",
    "        self.canvas.font = \"10px serif\"\n",
    "        display(self.canvas)        \n",
    "        \n",
    "        \n",
    "    def update_canvas(self, sleeptime=0.02):\n",
    "        #draw the scene\n",
    "        with hold_canvas():\n",
    "            # Clear the old animation step\n",
    "            self.canvas.clear()\n",
    "\n",
    "            y = 100\n",
    "            size = 5\n",
    "\n",
    "            self.canvas.stroke_text(\"time:%d\"%self.timestep, 10, 10)\n",
    "            #self.canvas.stroke_text(\"#fruits:%d\"%len(self.fruits), 10, 30)\n",
    "            self.canvas.stroke_text(\"#score:%d\"%self.score, 10, 30)\n",
    "            self.canvas.stroke_text(\"#found:%d\"%self.countfruits, 10, 50)\n",
    "\n",
    "            self.canvas.stroke_style = \"blue\"\n",
    "            for (x) in self.trees:\n",
    "                self.canvas.stroke_rect(x, y, size, size)\n",
    "\n",
    "            self.canvas.fill_style = \"red\"\n",
    "            xs = [x for (x,t) in self.fruits]\n",
    "            ys = [100]*len(xs)\n",
    "            self.canvas.fill_circles(xs, ys, size) #use vectorized version\n",
    "\n",
    "            self.canvas.stroke_style = \"green\"\n",
    "            self.canvas.stroke_rect(self.agent, y, size, size)\n",
    "\n",
    "\n",
    "        # Animation frequency ~50Hz = 1./50. seconds\n",
    "        if sleeptime>0:\n",
    "            sleep(sleeptime)\n",
    "\n",
    "\n",
    "    def play(self, model):\n",
    "        self.init_canvas()\n",
    "        terminated=False\n",
    "        obs = self.reset()\n",
    "        while not terminated:\n",
    "            action = choose_action(model, obs)\n",
    "            obs, reward, terminated = self.step(action)\n",
    "            self.update_canvas(sleeptime=0)        \n",
    "            \n",
    "e = Env(500)\n",
    "\n",
    "e.init_canvas()\n",
    "terminated=False\n",
    "obs = e.reset()\n",
    "while not terminated:\n",
    "    \n",
    "    #policy:\n",
    "    xt = obs[0]\n",
    "    if abs(xt)>50:\n",
    "        action = 1 if xt>0 else 0\n",
    "    else:\n",
    "        action = random.randint(0,1)\n",
    "    \n",
    "    #action = random.randint(0,1)\n",
    "\n",
    "    obs, reward, terminated = e.step(action)\n",
    "    e.update_canvas()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e1362c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "84443ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d95b1cee235445aadb375784c1489d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RoughCanvas(height=200, width=1000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Env():\n",
    "    \n",
    "    def __init__(self, num_steps):\n",
    "        self.num_steps = num_steps\n",
    "        self.num_trees = 2\n",
    "        self.num_fruits = 40\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.trees = [(random.randrange(100,300)), (random.randrange(400,600))]\n",
    "\n",
    "        self.fruits = np.zeros((self.num_fruits, 3))\n",
    "        fruits = []\n",
    "        for xt in self.trees:\n",
    "            fruits.extend(xt+np.round(100*truncnorm.rvs(-1, 1, size=int(self.num_fruits/self.num_trees))))\n",
    "        self.fruits[:,0]=fruits\n",
    "        self.fruits[:,1]=np.random.uniform(10,50,self.num_fruits)  #number of time steps before fruit's location changes\n",
    "        self.fruits[:int(self.num_fruits/2),2]=self.trees[0] #remember the tree where the next fruit will fall\n",
    "        self.fruits[int(self.num_fruits/2):,2]=self.trees[1]\n",
    "        \n",
    "        self.agent = random.randrange(0., 800.)\n",
    "        self.countfruits = 0\n",
    "        self.score = 0.\n",
    "        self.timestep = 0\n",
    "        \n",
    "        return self.obs()\n",
    "\n",
    "    def obs(self):\n",
    "        o = [(xt - self.agent) for (xt) in self.trees]\n",
    "        return o\n",
    "    \n",
    "        \n",
    "    def step(self, action):\n",
    "\n",
    "        self.timestep += 1\n",
    "        \n",
    "        #replace old fruit\n",
    "        self.fruits[:,1] -= 1 #age all fruits\n",
    "        old = np.where(self.fruits[:,1]<=0, True, False) #find fruits with expired shelf life...\n",
    "        self.fruits[:,0] = np.where(old, self.fruits[:,2]+np.round(100*truncnorm.rvs(-1,1)), self.fruits[:,0]) #drop a new fruit near the same tree\n",
    "        self.fruits[:,1] = np.where(old, np.random.uniform(10,50), self.fruits[:,1]) #and give it a new shelf life\n",
    "\n",
    "        #move the agent\n",
    "        direction = -1 if action==0 else +1\n",
    "        distance = direction * random.randint(1,5)\n",
    "        self.agent += distance  #change the position\n",
    "        \n",
    "        #calculate the reward\n",
    "        \n",
    "        self.score -= abs(distance)  #consume energy, negative reward, for moving\n",
    "        \n",
    "        found = np.where(self.agent==self.fruits[:,0], True, False)\n",
    "        count = np.count_nonzero(found==True)\n",
    "        self.score += 1000 * count #it is possible that several fruits have the same location\n",
    "        self.countfruits += count\n",
    "        if count>0:\n",
    "            self.fruits[:,0] = np.where(found, self.fruits[:,2]+np.round(100*truncnorm.rvs(-1,1)), self.fruits[:,0]) #drop a new fruit near the same tree\n",
    "            self.fruits[:,1] = np.where(found, np.random.uniform(10,50), self.fruits[:,1]) #and give it a new shelf life\n",
    "        \n",
    "        o = self.obs()\n",
    "        reward = self.score\n",
    "        terminated = True if self.timestep>=self.num_steps else False\n",
    "        if self.agent<0 or self.agent>1000:\n",
    "            terminated = True\n",
    "            score = -10000 #discourage suicidal policies\n",
    "        \n",
    "        return o, reward, terminated   \n",
    "\n",
    "    \n",
    "    def init_canvas(self):\n",
    "        #canvas = Canvas(width=1000, height=200)\n",
    "        self.canvas = RoughCanvas(width=1000, height=200)\n",
    "        self.canvas.font = \"10px serif\"\n",
    "        display(self.canvas)        \n",
    "        \n",
    "        \n",
    "    def update_canvas(self, sleeptime=0.02):\n",
    "        #draw the scene\n",
    "        with hold_canvas():\n",
    "            # Clear the old animation step\n",
    "            self.canvas.clear()\n",
    "\n",
    "            y = 100\n",
    "            size = 5\n",
    "\n",
    "            self.canvas.stroke_text(\"time:%d\"%self.timestep, 10, 10)\n",
    "            #self.canvas.stroke_text(\"#fruits:%d\"%len(self.fruits), 10, 30)\n",
    "            self.canvas.stroke_text(\"#score:%d\"%self.score, 10, 30)\n",
    "            self.canvas.stroke_text(\"#found:%d\"%self.countfruits, 10, 50)\n",
    "\n",
    "            self.canvas.stroke_style = \"blue\"\n",
    "            for (x) in self.trees:\n",
    "                self.canvas.stroke_rect(x, y, size, size)\n",
    "\n",
    "            self.canvas.fill_style = \"red\"\n",
    "            xs = self.fruits[:,0]\n",
    "            ys = [100]*self.num_fruits\n",
    "            self.canvas.fill_circles(xs, ys, size) #use vectorized version\n",
    "\n",
    "            self.canvas.stroke_style = \"green\"\n",
    "            self.canvas.stroke_rect(self.agent, y, size, size)\n",
    "\n",
    "\n",
    "        # Animation frequency ~50Hz = 1./50. seconds\n",
    "        if sleeptime>0:\n",
    "            sleep(sleeptime)\n",
    "\n",
    "\n",
    "    def play(self, model):\n",
    "        self.init_canvas()\n",
    "        terminated=False\n",
    "        obs = self.reset()\n",
    "        while not terminated:\n",
    "            action = choose_action(model, obs)\n",
    "            obs, reward, terminated = self.step(action)\n",
    "            self.update_canvas(sleeptime=0)        \n",
    "            \n",
    "e = Env(500)\n",
    "\n",
    "e.init_canvas()\n",
    "terminated=False\n",
    "obs = e.reset()\n",
    "while not terminated:\n",
    "    \n",
    "    #policy:\n",
    "    xt = obs[0]\n",
    "    if abs(xt)>50:\n",
    "        action = 1 if xt>0 else 0\n",
    "    else:\n",
    "        action = random.randint(0,1)\n",
    "    \n",
    "    #action = random.randint(0,1)\n",
    "\n",
    "    obs, reward, terminated = e.step(action)\n",
    "    e.update_canvas()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca9edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4ff27929-3e5e-4c29-ac64-437c3a03dc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rl_model(n_actions):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=n_actions, activation=None)  #returns logits (un-normalized log-proba for each action)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "#a logbook to remember observations, actions and rewards for an entire episode\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "    \n",
    "    def clear(self):\n",
    "        self.observations=[]\n",
    "        self.actions=[]\n",
    "        self.rewards=[]\n",
    "        \n",
    "    def add_to_memory(self, new_obs, new_action, new_reward):\n",
    "        self.observations.append(new_obs)\n",
    "        self.actions.append(new_action)\n",
    "        self.rewards.append(new_reward)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.actions)\n",
    "\n",
    "def choose_action(model, observation, single=True):\n",
    "\n",
    "    # add batch dimension to the observation if only a single example was provided\n",
    "    observation = np.expand_dims(observation, axis=0) if single else observation\n",
    "    logits = model.predict(observation, verbose=0)\n",
    "    action = tf.random.categorical(logits, num_samples=1)  #randomly pick an action - tf's categorical takes unornmalized log proba as input\n",
    "    action = action.numpy().flatten()\n",
    "    return action[0] if single else action\n",
    "\n",
    "def normalize(x):\n",
    "    x -= np.mean(x)\n",
    "    x /= np.std(x)\n",
    "    return x.astype(np.float32)\n",
    "\n",
    "def discount_rewards(rewards, gamma=0.95):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    R=0\n",
    "    for t in reversed(range(0, len(rewards))):\n",
    "        R = R*gamma + rewards[t]\n",
    "        discounted_rewards[t]=R\n",
    "    return normalize(discounted_rewards)\n",
    "\n",
    "def compute_loss(logits, actions, rewards):\n",
    "    neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=actions)\n",
    "    loss = tf.reduce_mean(neg_logprob*rewards)\n",
    "    return loss\n",
    "\n",
    "def train_step(model, loss_function, optimizer, observations, actions, discounted_rewards):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(observations)\n",
    "        loss = loss_function(prediction, actions, discounted_rewards)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    grads, _ = tf.clip_by_global_norm(grads, 2.0)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e385fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = Env(500) #each episode lasts 500 time steps\n",
    "model = create_rl_model(n_actions=2)\n",
    "\n",
    "#env.play(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "95b828b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.20321432>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = Memory()\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "obs = env.reset()\n",
    "memory.clear()\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    action = choose_action(model, obs)\n",
    "    next_obs, reward, terminated = env.step(action)\n",
    "    memory.add_to_memory(obs, action, reward)\n",
    "    obs = next_obs\n",
    "    \n",
    "\n",
    "loss = train_step(model, compute_loss, optimizer, \n",
    "           observations=np.vstack(memory.observations),\n",
    "           actions=np.array(memory.actions),\n",
    "           discounted_rewards = discount_rewards(memory.rewards))\n",
    "\n",
    "score = reward\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7823f092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 score: 29507.0 loss: 0.13791351 time remaining: 45.027863121032716\n",
      "1 score: 16500.0 loss: 0.06365309 time remaining: 44.29214537143707\n",
      "2 score: 23481.0 loss: 0.11886824 time remaining: 43.52128487825394\n",
      "3 score: 18514.0 loss: 0.04055286 time remaining: 42.918258583545686\n",
      "4 score: 21502.0 loss: 0.14537612 time remaining: 42.36098740975062\n",
      "5 score: 25484.0 loss: 0.020437947 time remaining: 41.85362351735433\n",
      "6 score: 37493.0 loss: 0.20149529 time remaining: 41.47461810282299\n",
      "7 score: 22495.0 loss: 0.16536476 time remaining: 40.99527920484543\n",
      "8 score: 25563.0 loss: 0.2041778 time remaining: 40.50240138371785\n",
      "9 score: 38498.0 loss: 0.028194826 time remaining: 40.04234384377798\n",
      "10 score: 20460.0 loss: 0.021055793 time remaining: 39.7683297178962\n",
      "11 score: 13493.0 loss: 0.01543826 time remaining: 39.2891383767128\n",
      "12 score: 21545.0 loss: 0.18341012 time remaining: 38.72470948971235\n",
      "13 score: 27474.0 loss: 0.15639982 time remaining: 38.18946231206258\n",
      "14 score: 14476.0 loss: 0.19646773 time remaining: 37.661755201816554\n",
      "15 score: 37478.0 loss: 0.18473107 time remaining: 37.18965576539437\n",
      "16 score: 20531.0 loss: 0.16010553 time remaining: 36.69154949982961\n",
      "17 score: 32501.0 loss: 0.10580201 time remaining: 36.24903062714471\n",
      "18 score: 26520.0 loss: 0.12784573 time remaining: 35.82594796711938\n",
      "19 score: 40522.0 loss: 0.020979183 time remaining: 35.394327503045396\n",
      "20 score: 29500.0 loss: 0.04019498 time remaining: 34.99859289612088\n",
      "21 score: 17497.0 loss: 0.141428 time remaining: 34.57421461741129\n",
      "22 score: 16498.0 loss: 0.035062406 time remaining: 34.1445103612499\n",
      "23 score: 11513.0 loss: 0.052344956 time remaining: 33.725326240062714\n",
      "24 score: 30470.0 loss: 0.06912099 time remaining: 33.31985855150223\n",
      "25 score: 15514.0 loss: 0.20362991 time remaining: 32.89154280943748\n",
      "26 score: 24456.0 loss: 0.013881249 time remaining: 32.46628415805322\n",
      "27 score: 21490.0 loss: 0.08615722 time remaining: 32.040947914123535\n",
      "28 score: 10495.0 loss: 0.11140631 time remaining: 31.608754979056872\n",
      "29 score: 28499.0 loss: 0.13908532 time remaining: 31.201538639068602\n",
      "30 score: 18481.0 loss: 0.019932969 time remaining: 30.774941964175113\n",
      "31 score: 13517.0 loss: 0.024667332 time remaining: 30.342697847634554\n",
      "32 score: 13505.0 loss: 0.082159296 time remaining: 29.917106108954457\n",
      "33 score: 18525.0 loss: 0.026466006 time remaining: 29.488958605130513\n",
      "34 score: 7479.0 loss: 0.18780321 time remaining: 29.07344058468228\n",
      "35 score: 29515.0 loss: 0.055961248 time remaining: 28.645593569013805\n",
      "36 score: 19536.0 loss: 0.049536403 time remaining: 28.66682062116829\n",
      "37 score: 17499.0 loss: 0.12342568 time remaining: 28.20786435980546\n",
      "38 score: 20467.0 loss: 0.077937886 time remaining: 27.764967841368456\n",
      "39 score: 17465.0 loss: 0.06514606 time remaining: 27.307005172570545\n",
      "40 score: 9477.0 loss: 0.18986718 time remaining: 26.85416454067075\n",
      "41 score: 12547.0 loss: 0.10056331 time remaining: 26.400907539186026\n",
      "42 score: 13490.0 loss: 0.12048176 time remaining: 25.962939793379732\n",
      "43 score: 21535.0 loss: 0.079330415 time remaining: 25.515046520305404\n",
      "44 score: 14528.0 loss: 0.08208808 time remaining: 25.065602661768594\n",
      "45 score: 27458.0 loss: 0.05405349 time remaining: 24.61616416875867\n",
      "46 score: 21433.0 loss: 0.13076161 time remaining: 24.170448307027208\n",
      "47 score: 9520.0 loss: 0.012348887 time remaining: 23.73300006687641\n",
      "48 score: 35508.0 loss: 0.014121806 time remaining: 23.28607425357209\n",
      "49 score: 28493.0 loss: 0.14773445 time remaining: 22.84160369682312\n",
      "50 score: 33448.0 loss: 0.10425502 time remaining: 22.397742108503977\n",
      "51 score: 22497.0 loss: 0.07840444 time remaining: 21.95914986806038\n",
      "52 score: 18580.0 loss: 0.073007345 time remaining: 21.516626667751456\n",
      "53 score: 21498.0 loss: 0.0077307853 time remaining: 21.0753075811598\n",
      "54 score: 42517.0 loss: 0.028224196 time remaining: 20.6338699242563\n",
      "55 score: 35550.0 loss: 0.14202876 time remaining: 20.193147827472004\n",
      "56 score: 20461.0 loss: 0.07929381 time remaining: 19.758416834630463\n",
      "57 score: 20536.0 loss: 0.0727179 time remaining: 19.317942301158247\n",
      "58 score: 11574.0 loss: 0.14247969 time remaining: 18.877509271694443\n",
      "59 score: 43516.0 loss: 0.16678776 time remaining: 18.43728471159935\n",
      "60 score: 12533.0 loss: 0.12398511 time remaining: 17.999152353747945\n",
      "61 score: 23514.0 loss: 0.038098924 time remaining: 17.562701530353998\n",
      "62 score: 19469.0 loss: 0.06996605 time remaining: 17.122587598694693\n",
      "63 score: 11486.0 loss: -0.03177788 time remaining: 16.6825380521516\n",
      "64 score: 23473.0 loss: 0.0625191 time remaining: 16.24284579497117\n",
      "65 score: 17502.0 loss: 0.08437381 time remaining: 15.807455281777814\n",
      "66 score: 10529.0 loss: 0.12325006 time remaining: 15.367139233878596\n",
      "67 score: 15509.0 loss: -0.011155165 time remaining: 14.928285769621532\n",
      "68 score: 45521.0 loss: 0.13797083 time remaining: 14.489044554164444\n",
      "69 score: 11473.0 loss: 0.039304223 time remaining: 14.04995491754441\n",
      "70 score: 18465.0 loss: 0.11685522 time remaining: 13.61507462828372\n",
      "71 score: 31576.0 loss: 0.05594453 time remaining: 13.175625016291937\n",
      "72 score: 23528.0 loss: 0.042820137 time remaining: 12.736665754285578\n",
      "73 score: 529.0 loss: -0.019145792 time remaining: 12.298194683994259\n",
      "74 score: 30502.0 loss: -0.036751166 time remaining: 11.86219944190979\n",
      "75 score: 29503.0 loss: 0.14523223 time remaining: 11.423092948031007\n",
      "76 score: 12510.0 loss: 0.06314967 time remaining: 10.98391369952784\n",
      "77 score: 24519.0 loss: 0.16741778 time remaining: 10.545665350938455\n",
      "78 score: 6471.0 loss: -0.011598842 time remaining: 10.1068912611732\n",
      "79 score: 21508.0 loss: 0.102133974 time remaining: 9.670459445019564\n",
      "80 score: 20573.0 loss: 0.12350563 time remaining: 9.231818642586838\n",
      "81 score: 33473.0 loss: 0.12285078 time remaining: 8.793376130786367\n",
      "82 score: 13524.0 loss: 0.06604578 time remaining: 8.354125123234637\n",
      "83 score: 24443.0 loss: 0.11625124 time remaining: 7.917311012744904\n",
      "84 score: 17525.0 loss: 0.1591534 time remaining: 7.4782508651415505\n",
      "85 score: 15547.0 loss: 0.060413927 time remaining: 7.039025822720786\n",
      "86 score: 31494.0 loss: 0.09416794 time remaining: 6.5999108525528305\n",
      "87 score: 27480.0 loss: 0.059184678 time remaining: 6.160459746104298\n",
      "88 score: 22515.0 loss: 0.027423099 time remaining: 5.722277588969313\n",
      "89 score: 19480.0 loss: 0.011052261 time remaining: 5.282743262714809\n",
      "90 score: 43469.0 loss: 0.08692343 time remaining: 4.843421228901371\n",
      "91 score: 18513.0 loss: 0.109387845 time remaining: 4.403637141421222\n",
      "92 score: 32535.0 loss: 0.1342603 time remaining: 3.964883778941247\n",
      "93 score: 20563.0 loss: 0.12785743 time remaining: 3.524881964203314\n",
      "94 score: 38512.0 loss: 0.17123207 time remaining: 3.084590579208575\n",
      "95 score: 30410.0 loss: 0.13273527 time remaining: 2.6443583202858765\n",
      "96 score: 6534.0 loss: 0.11564325 time remaining: 2.2039035740996553\n",
      "97 score: 9505.0 loss: 0.054447196 time remaining: 1.7636950520431105\n",
      "98 score: 16480.0 loss: 0.08872786 time remaining: 1.3229952462995895\n",
      "99 score: 14504.0 loss: 0.10663836 time remaining: 0.882114862203598\n"
     ]
    }
   ],
   "source": [
    "memory = Memory()\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "num_episodes=100\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    memory.clear()\n",
    "    terminated = False\n",
    "    \n",
    "    #run the episode, keeping the model constant\n",
    "    while not terminated:\n",
    "        action = choose_action(model, obs)\n",
    "        next_obs, reward, terminated = env.step(action)\n",
    "        memory.add_to_memory(obs, action, reward)\n",
    "        obs = next_obs\n",
    "\n",
    "    score = reward\n",
    "    \n",
    "    loss = train_step(model, compute_loss, optimizer, \n",
    "               observations=np.vstack(memory.observations),\n",
    "               actions=np.array(memory.actions),\n",
    "               discounted_rewards = discount_rewards(memory.rewards))\n",
    "    \n",
    "    end = time.time()\n",
    "    print(i_episode, \"score:\", score, \"loss:\", loss.numpy(), \"time remaining:\", (num_episodes-i_episode+1)*(end-start)/(i_episode+1)/60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "940861a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rl_20221219-112929\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: rl_20221219-112929\\assets\n"
     ]
    }
   ],
   "source": [
    "model_file = 'rl_'+time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(model_file)\n",
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f17f5b-1470-4dbf-979b-88357106edc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6393e1a363042519ebe44570871eee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RoughCanvas(height=200, width=1000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = Env(500)\n",
    "env.play(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc176c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
